---
title: 慢盘问题：从青铜到王者的处理思路
date: 2023-08-11
description: ""
draft: false
tags:
  - book
---


大家好我是小义同学，这是<font color="#00b050">大厂面试拆解——项目实战</font>系列的第<font color="#953734">4</font>篇文章，

知识地图：数据可靠性-磁盘亚健康处理


为了解决故障后数据一致性问题，数据需要持久化存储，
（如何证明用户写入数据 和存储数据一致性的）

我们视角先离开CAP，ACID,MQ，分布式这样概念。

**这里假设单机，单机情况下遇到常见盲区（盲区定义别人知道，自己不知道）
1. 自己一直在用，一直在说，在工作总必然发生，但是系统都封装好了，根本不知道怎么回事。
2. 在生产环境偶尔出现，但是别人完全解决了，自己根本不知道这样情况。


本文慢盘为例子，看看不同系统怎么处理。

1. 慢盘 危害,常见的解决方法？
- IO抖动，严重影响业务。
- IO hang 服务不可用，致命。
- iostat命令检测
- 组RAID（业务保持不变）
- 标记节点下线

1. 普通业务的产品是如何选择？
- 硬件：组RAID
- Bcache--用SSD作为HDD盘缓存
- 软件：慢盘检测 +标记线下。


------------------------------------------------------------
## 慢盘有什么危害

##### 环境信息：

| **机器配置** | **详情**                                |
| -------- | ------------------------------------- |
| 场景       | 企业级服务器                                |
| **CPU**  | Intel(R) Xeon(R) Gold 5218R @ 2.30GHz |
| 内存       | 192G（6 * 32G）                         |
| 网卡       | 1 张 千兆 + 1张万兆                         |
| 磁盘       | HDD：36 个盘<br>SSD： 2个 NVME             |
##### 从os角度说对业务产生影响

1. 单个节点磁盘异常 场景下的IO抖动问题，影响业务.
2. I/O hang:IO hang是指在系统运行过程中，因某些IO耗时过长而引起的系统不稳定甚至宕机。 
3. IO hang 发生在I/O 路径的不同层， 往往表现为进程进入不可中断睡眠（D态），系统整体 I/O 吞吐骤降或“卡死”。

## 目前有哪些解决方案

### 方法1： 一点不神奇，直接用命令,写shell脚本

青铜真实感受：
1. 36个盘 都上架到机柜了，我怎么知道那个盘有问题，放到我面前我也不知道，开始躲避。
2. 最大问题是什么，简单一个命令，老板不会指派我处理的，因为没有积累足够信任，客户很重要，必须信任人才可以。
3. 这是典型的，想的太多，不出活。多检查自己写脚本 这个认真仔细。其他就是冲冲冲等着解决问题呢。
思路：
>1. 写shell脚本，利用smartctl统计等评估磁盘健康状态，及时发现磁盘损坏,。
>2. 无法提前预测磁盘故障，属于后知后觉要求 
>3. 不要看简单粗暴不<font color="#245bdb">从网络查询命令,手写命令,</font>事故发生了,必须马上5分钟手写出来。就是命令，练出来，<font color="#245bdb">自己平时多手敲 手敲 手敲几次。变成肌肉记忆。shell的for循环 命令执行2个要点。</font>
>
##### smartctl命令

是Linux下的一款用于监控和管理硬盘健康状态的工具

用法：
1. 无检测云服务商主机的虚拟磁盘 例如 在 KVM 中 QEMU 虚拟出来的virtio 磁盘。

扩展阅读
1. KVM（Kernel-based Virtual Machine）是一种基于Linux内核的开源硬件虚拟化技术。它利用Linux内核的虚拟化模块，将物理服务器划分为多个虚拟机。
2. **KVM虚拟化架构**
	KVM虚拟化架构主要包括以下几个部分：
	- **KVM内核模块**：负责虚拟CPU和内存的管理。
	- **QEMU（Quick Emulator）**：作为KVM的虚拟机监控程序，提供虚拟机的用户空间支持，模拟各种硬件设备。
	- **Libvirt**：管理虚拟机和虚拟化功能的工具，提供统一的API和命令行接口。

 
 lsblk - list block devices

##### iostat 命令

|Device|r/s|w/s|r_await|w_await|await|%util|
|---|---|---|---|---|---|---|
|sda|50.0|20.0|4.00|2.50|3.33|30.0|
- **读请求**：每秒 50 次，平均每次读耗时 4 ms。
- **写请求**：每秒 20 次，平均每次写耗时 2.5 ms。
- **整体平均**：综合读写后，每次 I/O 平均等待 3.33 ms。
- **设备利用率**：30%，说明还有较多空闲带宽。



关键指标：

1.  **await**: is the average time (in milliseconds) for I/O requests issued to the device to be served.
2. 它既包含了一个 IO 在 IO 软件栈中处理的时间，也包含了磁盘的处理时间。
3. 因此 await 冲高现象是 IO 抖动问题中比较典型的特征。

	r_await = （该周期内所有已完成读请求的等待时间总和） 
	           ÷ （该周期内已完成的读请求总数）
	w_await = （该周期内所有已完成写请求的等待时间总和） 
	           ÷ （该周期内已完成的写请求总数）

- **低值**（通常 < 5 ms）：说明磁盘响应迅速，I/O 子系统运行良好。
- **中等值**（5–20 ms）：在机械盘上较为常见，或 SSD 在负载较高时可能出现。
- **高值**（> 20 ms）：可能代表磁盘瓶颈、排队积压，或下层（如 RAID 控制器、网络存储）出现延

	对于 HDD：
	每一秒执行一次 iostat 命令，await 超过 3s 为一次慢盘。
	
	对于 SSD：
	每一秒执行一次 iostat 命令，await 超过 1.5s 为一次慢

### 方法 2： 本地磁盘（云盘）型组RAID

青铜真实感受：
- 我对软件很多副本，一致性协议，负载均衡，这个我还没搞明白你，远离RAID这个事情，<font color="#245bdb">对眼前问题不管不问，鸵鸟策略</font>。
- 没有大量生产环境实践操作，开始动手操作，<font color="#245bdb">做事方法完全不对 ，别人根本不是这样做的</font>。

王者视角：
- <font color="#245bdb">不用写一行代码</font>，购买磁盘 ，购买网卡，这不完成了，至于如何生产的 我不关心，有问题找厂家，前端业务完全感知不到“慢盘”对业务的影响，这样业务改造成本 ，双手赞成。
- 我需要了解的什么软RAID，什么硬RAID，
- 我需要了解RAID里奇偶校验--异或运算。
- 与多副本相比，RAID组有什么提升，例如硬件寿命，读写性能 这些对比数据 这个才是我最关心的，
- 多压测，给出最佳实践才是最重要的。本地RAID 绝对超过多副本垮节点一致性，拿出数据来，找老板要资源去。

#### 什么是RAID

RAID是Redundant Array of Independent Disks（“独立磁盘冗余阵列”）的缩写。
它是一种将多个硬盘相结合以存储数据的技术，是目前商用服务器常见的磁盘管理技术。


![RAID阵列类型](https://money-1256465252.cos.ap-beijing.myqcloud.com/mac/20250425122454.png)

有人说  **双重RAID是迄今为止对付“慢盘”效应最有效的办法。**
1. 硬件故障隔离，避免出现Ceph三副本 随机读写慢盘/满盘，情况出现。Ceph还真的就是这样设计的，因为Ceph没法保证新的对象是否落在空盘而不落在满盘，所以Ceph选择在有盘满了时，就拒绝服务.
2. **在线**替换更大容量的NVMe固态硬盘，缓存加速。


#### RAID阵列类型
1. **RAID 0** ：设计的目标是为了提升读写性能单块盘的 N 倍，不具有冗余信息，任何一块磁盘损坏，整个 RAID 不可用
2. **RAID 1**：镜像(Mirroring)，为每份数据都提供一份或多分数据冗余,至少需要两个硬盘，只要其中一个硬盘运行正常。占用N倍空间
3. RAID 1+0（RAID10）: RAID 1+0 由于兼具可靠性和好的性能，在商业应用中很广泛
4. RAID 2 的设计目标就是在 RAID 0 级别的基础上，加入了海明纠错码，<font color="#245bdb">目前已经基本被淘汰</font>了
5. RAID 3 就是把数据按照**字节**分别存在不同的磁盘中，并且最后一个磁盘提冗余纠错。XOR 算法。但是却无法做到并发 I/O。
6. RAID 4 是把数据按照**分块**分别存在不同的磁盘中，并且最后一个磁盘提供纠错冗余，RAID 4 面临淘汰
7. RAID 5 是把数据块按照分块分别存在不同的磁盘中，并且冗余信息也会分块分布在多块磁盘中。
	RAID 5 可以理解为是 RAID 0 和 RAID 1 的折中方案。
	RAID 5 可以为系统提供数据安全保障，但保障程度要比镜像低而磁盘空间利用率比镜像高。****
8. RAID 6 是把数据块按照分块分别存在不同的磁盘中，并且冗余信息为两份奇偶校验码，分布在多块磁盘中。坏掉一块盘，RAID 还能正常工作
9.  2个类型：软 RAID 和硬 RAID



### 方法 3：通过高可用集群，让部分节点下线




## 别人怎么选择的

>分布式存储的性能不是由最快的磁盘决定，而是由最慢的磁盘所决定,木桶效应.
>
### 1.  场景1：Bcache--用SSD代替内存，作为HDD盘缓存 

**Bcache**：
>Bcache is a Linux kernel block layer cache. 
>It allows one or more fast disk drives such as flash-based solid state drives (SSDs) to act as a cache for one or more slower hard disk drives.

场景：
- 使用SSD盘在IO速度较慢的HDD盘上面做一层缓存，从而来提高HDD盘的IO速率
- bcache 执行的缓存操作在块设备级别进行.
- 经常访问的热数据会缓存在固态硬盘中并直接返回给应用程序

原理：
- 缓存的实现方式是使用 SSD 来存储与执行的随机读写相关的数据，接近于零的寻道时间是 SSD 最显著的特性
- 缓存写入操作支持回写和直写 （默认）两种策略

特点：
- 稳定 - 现已投入生产使用。
- 单个缓存设备可用于缓存任意数量的后备设备
- 从非正常关机中恢复 - 直到缓存与后备设备保持一致，写入才会完成（bcache 内部不区分正常关机和非正常关机）。

### 2. 场景2：去中心化架构Ceph ，盘坏--下线对应的存储服务。

去中心化架构以Ceph为代表 

其主要技术特征在于：
1. 业务数据切分为固定大小的数据块（如4MB大小的Object），
2. 通过伪随机CRUSH算法计算得出该数据块的位置。这个不需要元数据存储。
3. 尽可能将数据均匀分布在各个节点各个磁盘上。


解决方式：那个盘故障，下线那个osd
1. 检测到盘故障
2. 通知mon下线osd
3. osd准备下线


### 3. Google GFS 

中心化架构以Google GFS 等分布式文件系统为代表
其主要技术特征在于：
- 采用中心化元数据服务器来保存和查询数据块的具体位置信息，
- 而数据块由元数据服务器按照某种条带（Strip）算法或伪随机算法为数据块分配地址空间

解决办法：
 - 我估计不是处理，本来运行廉价设备上。

### 4. 数据库场景：事后发现，节点缩容。

问题描述： 一次SSD磁盘寿命耗尽导致的TiDB集群写入变慢问题处理

背景：
最近处理了一起由于 SSD 磁盘寿命耗尽导致的 xxx 集群写入变慢的问题，
服务器磁盘是 SATA SSD、RAID 10。

排除过程：
- iops 为0 ，也就是在一秒内，磁盘进行多少次 I/O 读写
-  缩容操作


## 总结

-  熟悉iostat -dxm  命令，看await指标
-  了解io流程，每个io步骤到 io hang
- 分布式存储的性能不是由最快的磁盘决定，而是由最慢的磁盘所决定
-  本地：Bcache 和RAID方案
- 分布式集群 高可用特性，支持节点缩容。


如果您觉得阅读本文对您有帮助，
请点一下“**点赞” 按钮，
您的“点赞” 将是我最大的写作动力

## 参考资料

##### 工程师如何在工作中提升自己?
   - 个人的成长70%来自于岗位实践，20%来自向他人学习，10%来自于培训。
   - 离日暮西山的行业，做有积累的事情，一边走一边看，切勿一条道走到黑
   - 有清晰的结果导向思维。功劳和苦劳不是一回事

##### 写给工程师的十条精进原则
- 原则二：时间观念
- 工作安排要有计划性：精确的开发时间，进而再合理地安排开发、联调、测试计划，关键时间点要可检查
- 工作安排要分清楚主次：要学会分辨出这些干扰的工作项，保证重要紧急的事情能够按时交付，避免这种“本末倒置”的工作方式 
#####  程序员需要知道的RAID基本原理
#####  风云再续：他抖任他抖，IO诊断在我手
-  如何对付臭名昭著的 IO 夯
- 一个 IO 的生命周期
- 史诗级的IO架
##### wiki/Bcache



