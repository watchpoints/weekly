---
title: ã€ç¿»è¯‘ã€‘Crimsonï¼šç”¨äºå¤šæ ¸å¯æ‰©å±•æ€§çš„ä¸‹ä¸€ä»£ Ceph OSD
date: 2025-04-27
description: 
draft: false
tags:
  - CEPH
categories:
  - CEPH
---

# Crimsonï¼šç”¨äºå¤šæ ¸å¯æ‰©å±•æ€§çš„ä¸‹ä¸€ä»£ Ceph OSD

## ç›®å½•
1. å‰è¨€ä¸æ€è€ƒ
2. èƒŒæ™¯ä¸æŠ€æœ¯æ¼”è¿›
3. Crimson vs Classic OSD æ¶æ„
4. Seastar æ¡†æ¶ç®€ä»‹
5. è¿è¡Œåˆ°å®Œæˆæ€§èƒ½
6. å¤šåˆ†ç‰‡å®ç°
7. OSD ç»„ä»¶è¯¦è§£
8. ObjectStore åç«¯
    - AlienStore
    - CyanStore
    - SeaStore
9. æ€»ç»“ä¸æµ‹è¯•é…ç½®

---
åœ¨å±æœºæ—¶ä»£åšæŒä¿®ç‚¼å†…åŠŸï¼Œé”»ç‚¼èº«ä½“ï¼Œ
ç­‰å¾…æ’çºªå…ƒçš„åˆ°æ¥ï¼Œä¸€æ—¦æ–°æœºä¼šå‡ºç°ï¼Œ
æˆ‘ä»¬å°±å·²ç»åšå¥½äº†å‡†å¤‡ï¼ŒæŠ“ä½æ–°çš„æœºä¼šï¼Œå®ç°è‡ªå·±çš„ç›®æ ‡

ç³»ç»Ÿé‡æ„ï¼Œé¡¹ç›®ä¼˜åŒ–ï¼Œé‡æ–°å¼€å‘ç³»ç»Ÿ æœ€é‡è¦ä¸€ç‚¹æ˜¯ä»€ä¹ˆ?
Finer-grained Resource Managementï¼ˆç»†ç²’åº¦èµ„æºç®¡ç†ï¼‰â€‹
- â€‹ç›®æ ‡â€‹â€‹ï¼šå°†èµ„æºï¼ˆCPUã€å†…å­˜ã€I/Oï¼‰çš„åˆ†é…å•ä½ä»"ç²—ç²’åº¦"ï¼ˆå¦‚è¿›ç¨‹ã€çº¿ç¨‹ï¼‰ç»†åŒ–åˆ°"å¾®ç²’åº¦"ï¼ˆå¦‚åç¨‹ã€å†…å­˜å—ã€NUMAèŠ‚ç‚¹ï¼‰ã€‚
- Fast Path Implementationsï¼ˆå¿«é€Ÿè·¯å¾„å®ç°ï¼‰â€‹**â€‹
	- â€‹â€‹ç½‘ç»œåè®®æ ˆâ€‹ï¼šDPDK/XDPç»•è¿‡å†…æ ¸åè®®æ ˆï¼ˆèŠ‚çœÎ¼sçº§å»¶è¿Ÿï¼‰ TCP/IPåè®®æ ˆ
	- â€‹â€‹å­˜å‚¨æ ˆâ€‹â€‹ï¼šNVMe over Fabricsçš„`Zero-Copy RDMA`ã€‚
	- â€‹â€‹æ•°æ®åº“â€‹ï¼šLSM-Treeçš„MemTableç›´æ¥å†™å…¥ï¼ˆè·³è¿‡WALé˜Ÿåˆ—ï¼‰ã€‚

å†…æ ¸æ—è·¯ï¼ˆKernel Bypassï¼‰æ˜¯ä¸€ç§é«˜æ€§èƒ½è®¡ç®—å’Œç½‘ç»œé€šä¿¡é¢†åŸŸçš„é‡è¦æŠ€æœ¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç»•è¿‡æ“ä½œç³»ç»Ÿå†…æ ¸

## 1. å‰è¨€ä¸æ€è€ƒ

<font color="#245bdb">åœ¨å±æœºæ—¶ä»£åšæŒä¿®ç‚¼å†…åŠŸï¼Œé”»ç‚¼èº«ä½“ï¼Œ</font>
<font color="#245bdb">ç­‰å¾…æ’çºªå…ƒçš„åˆ°æ¥ï¼Œä¸€æ—¦æ–°æœºä¼šå‡ºç°ï¼Œ</font>
<font color="#245bdb">æˆ‘ä»¬å°±å·²ç»åšå¥½äº†å‡†å¤‡ï¼ŒæŠ“ä½æ–°çš„æœºä¼šï¼Œå®ç°è‡ªå·±çš„ç›®æ ‡</font>ã€‚

**åšæŒï¼ˆäººç”Ÿå¯†ç ï¼‰**

åšæŒçš„åŠ›é‡åœ¨äºï¼Œå®ƒè®©æˆ‘ä»¬åœ¨è¿½æ±‚ç›®æ ‡çš„è¿‡ç¨‹ä¸­ï¼Œä¸æ–­è¶…è¶Šè‡ªæˆ‘ï¼ŒæŒ–æ˜å‡ºè‡ªå·±çš„æ½œèƒ½ã€‚æ— è®ºæˆ‘ä»¬çš„ç›®æ ‡æœ‰å¤šå¤§æˆ–å¤šå°ï¼Œå®ç°å®ƒä»¬é€šå¸¸éœ€è¦æ—¶é—´ã€åŠªåŠ›å’Œæ¯…åŠ›ã€‚æ­£æ˜¯åšæŒï¼Œè®©æˆ‘ä»¬åœ¨é¢ä¸´æŒ‘æˆ˜æ—¶ä¸æ”¾å¼ƒï¼Œä»è€Œæœ€ç»ˆå®ç°æˆ‘ä»¬çš„ç›®æ ‡ã€‚

**ç‹¬ç«‹æ€è€ƒï¼ˆæŒæ¡æœªæ¥çš„å…³é”®èƒ½åŠ›ï¼‰**

åœ¨è¿™ä¸ªä¿¡æ¯çˆ†ç‚¸çš„æ—¶ä»£ï¼Œæˆ‘ä»¬æ¯å¤©éƒ½ä¼šæ¥è§¦åˆ°å¤§é‡çš„ä¿¡æ¯å’Œè§‚ç‚¹ã€‚é¢å¯¹è¿™äº›çº·ç¹å¤æ‚çš„ä¿¡æ¯ï¼Œå­¦ä¼šç‹¬ç«‹è¾©è¯æ€è€ƒæ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ç‹¬ç«‹è¾©è¯æ€è€ƒæ˜¯æˆ‘ä»¬åœ¨è®¤çŸ¥ä¸–ç•Œã€åšå†³ç­–å’Œè§£å†³é—®é¢˜æ—¶çš„å…³é”®èƒ½åŠ›ã€‚å®ƒèƒ½å¸®åŠ©æˆ‘ä»¬åœ¨èŒ«èŒ«ä¿¡æ¯æµ·æ´‹ä¸­æ‰¾åˆ°çœŸå®çš„ä»·å€¼ã€‚

**è§†é‡ï¼ˆæ‹“å®½äººç”Ÿçš„æ— é™å¯èƒ½ï¼‰**

åœ¨æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œä¸­ï¼Œè§†é‡çš„é‡è¦æ€§ä¸è¨€è€Œå–»ã€‚
å®ƒä¸ä»…æŒ‡çš„æ˜¯æˆ‘ä»¬çœ‹å¾…é—®é¢˜çš„è§’åº¦å’Œæ·±åº¦ï¼Œæ›´æ˜¯æˆ‘ä»¬ç†è§£å’Œæ¥è§¦ä¸–ç•Œçš„çª—å£ã€‚
ä¸€ä¸ªå¼€é˜”çš„è§†é‡å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£ä¸–ç•Œï¼Œæ›´å‡†ç¡®åœ°åˆ¤æ–­å½¢åŠ¿ï¼Œæ›´æœ‰æ•ˆåœ°è§£å†³é—®é¢˜ã€‚

---

## 2. èƒŒæ™¯ä¸æŠ€æœ¯æ¼”è¿›

### 2.1 å­˜å‚¨ç¡¬ä»¶çš„å˜åŒ–

ç”¨äºå­˜å‚¨è½¯ä»¶çš„è£¸æœºç¡¬ä»¶çš„æ¶æ„ä¸€ç›´åœ¨ä¸æ–­å˜åŒ–ã€‚

å•è¯ï¼š
- constantly ï¼šç»å¸¸
- metal é‡‘å±ï¼š
- Bare-metalâ€‹ï¼šè®¡ç®—æœºä»¶
- Technology landscapeâ€‹ï¼š æŠ€æœ¯å‘å±•è¶‹åŠ¿

On one hand, memory and IO technologies have been developing rapidly.
ä¸€æ–¹é¢ï¼Œå†…å­˜å’Œ IO æŠ€æœ¯ä¸€ç›´åœ¨è¿…é€Ÿå‘å±•

At the time when Ceph was originally designed, it was deployed generally on **spinning disks** capable of a few hundreds of IOPS with tens of gigabytes of disk capacity. 

åœ¨Cephæœ€åˆè®¾è®¡æ—¶ï¼Œå®ƒé€šå¸¸éƒ¨ç½²åœ¨æœºæ¢°ç¡¬ç›˜ä¸Šï¼Œè¿™äº›ç¡¬ç›˜ä»…èƒ½æä¾›æ•°ç™¾æ¬¡IOPSï¼ˆæ¯ç§’è¾“å…¥/è¾“å‡ºæ“ä½œæ¬¡æ•°ï¼‰çš„ååèƒ½åŠ›å’Œæ•°åGBçš„ç£ç›˜å®¹é‡ã€‚

å•è¯ï¼š
- spinning disksï¼šæœºæ¢°ç¡¬ç›˜
- a few hundreds of IOPSï¼šæ•°ç™¾IOPS
- Originallyï¼š**è®¾è®¡åˆè¡·**


**Modern NVMe devices** now can serve millions of IOPS and support terabytes of disk space.
ç°ä»£ NVMe è®¾å¤‡ç°åœ¨å¯ä»¥æä¾›æ•°ç™¾ä¸‡ IOPS å¹¶æ”¯æŒæ•° TB çš„ç£ç›˜ç©ºé—´


DRAM capacity has increased 128 times in about 20 years
And for Network IO, a NIC device is now capable of delivering speeds upwards of 400Gbps compared to 10Gbps just a few years ago.  

ç°ä»£ NVMe è®¾å¤‡ç°åœ¨å¯ä»¥æä¾›æ•°ç™¾ä¸‡ IOPS å¹¶æ”¯æŒæ•° TB çš„ç£ç›˜ç©ºé—´ã€‚
DRAM å®¹é‡åœ¨å¤§çº¦ 20 å¹´å†…å¢é•¿äº† 128 å€Â 
å¯¹äºç½‘ç»œ IOï¼ŒNIC è®¾å¤‡ç°åœ¨èƒ½å¤Ÿæä¾›è¶…è¿‡ 400Gbps çš„é€Ÿåº¦ï¼Œè€Œå‡ å¹´å‰ä¸º 10Gbpsã€‚

>å•¥æ„æ€ï¼šç£ç›˜ ç½‘ç»œï¼Œå†…å­˜
- ä»æœºæ¢°ç¡¬ç›˜ï¼ˆæ•°ç™¾IOPS/æ•°åGBï¼‰åˆ°NVMeï¼ˆç™¾ä¸‡çº§IOPS/TBçº§å®¹é‡ï¼‰
- ç½‘ç»œå¸¦å®½ä»10Gbpsè·ƒå‡è‡³400Gbps
- DRAMå®¹é‡20å¹´å¢é•¿128å€

ä½†æ˜¯ï¼šåœ¨Cephæœ€åˆè®¾è®¡æ—¶ï¼Œå®ƒé€šå¸¸éƒ¨ç½²åœ¨æœºæ¢°ç¡¬ç›˜ä¸Š

Cephä¸ºé€‚åº”ä»HDDåˆ°NVMeçš„ç¡¬ä»¶å˜é©ï¼Œä¸»è¦è¿›è¡Œäº†ä»¥ä¸‹å…³é”®æ¶æ„è°ƒæ•´ï¼š

1. â€‹**â€‹åˆ†å±‚å­˜å‚¨å¼•æ“ä¼˜åŒ–â€‹**â€‹

- å¼•å…¥â€‹**â€‹Bluestoreâ€‹**â€‹æ›¿ä»£Filestore
    - ç›´æ¥ç®¡ç†è£¸è®¾å¤‡ï¼Œè§„é¿æ–‡ä»¶ç³»ç»Ÿå¼€é”€ï¼ˆé’ˆå¯¹NVMeä½å»¶è¿Ÿç‰¹æ€§ï¼‰
    - å…ƒæ•°æ®ä¸æ•°æ®åˆ†ç¦»å­˜å‚¨ï¼ˆHDDæ—¶ä»£å…ƒæ•°æ®éœ€ç‰¹æ®Šä¼˜åŒ–
- 2. â€‹**â€‹IOè·¯å¾„é‡æ„â€‹**â€‹
- å®ç°â€‹**â€‹å¹¶è¡ŒåŒ–IOå¤„ç†â€‹**â€‹
    - HDDæ—¶ä»£ï¼šé¡ºåºIOä¼˜åŒ–ï¼ˆæœºæ¢°å¯»é“å»¶è¿Ÿæ˜¯ç“¶é¢ˆï¼‰
    - NVMeæ—¶ä»£ï¼šæ·±åº¦é˜Ÿåˆ—å¹¶å‘ï¼ˆåˆ©ç”¨å¤šæ ¸CPU+NVMeé«˜é˜Ÿåˆ—æ·±åº¦ï¼‰
    - æœºæ¢°ç¡¬ç›˜ï¼ˆHDDï¼‰æ—¶ä»£çš„ä¸²è¡ŒIO
    -  æ”¯æŒâ€‹**â€‹å¹¶è¡Œé€šé“â€‹**â€‹ï¼ˆå¦‚PCIe 4.0 x4 = 64æ¡è™šæ‹Ÿè½¦é“ï¼‰
    - é˜Ÿåˆ—æ·±åº¦å¯è¾¾â€‹**â€‹æ•°ä¸‡â€‹**â€‹ï¼ˆç›¸å½“äºåŒæ—¶å¤„ç†æ•°ä¸‡è¾†è½¦ï¼‰

 â€‹**â€‹ç½‘ç»œåè®®å¢å¼ºâ€‹**â€‹
- æ”¯æŒâ€‹**â€‹RDMA/RoCEv2â€‹**â€‹
    - åŒ¹é…400Gbpsç½‘å¡æ€§èƒ½
    - ä¼ ç»ŸTCP/IPæ ˆæ— æ³•æ»¡è¶³NVMe-oFåœºæ™¯



On the other hand, single-core performance has plateauedï¼ˆè¾¾åˆ°ç¨³å®šæ°´å¹³ï¼‰ as the CPU frequency and single-thread performance of CPU cores has remained in the same range for about a decade now
å¦ä¸€æ–¹é¢ï¼Œå•æ ¸æ€§èƒ½è¶‹äºåœæ»ï¼Œå› ä¸ºåœ¨è¿‡å»åå¹´ä¸­ï¼ŒCPU çš„ä¸»é¢‘å’Œå•çº¿ç¨‹æ€§èƒ½å‡ ä¹æ²¡æœ‰æ˜æ˜¾æå‡

In contrast, the number of logical cores has grown quickly with the increasing transistor ï¼ˆæ™¶ä½“ç®¡ï¼‰scale.  
ç›¸è¾ƒä¹‹ä¸‹ï¼Œéšç€æ™¶ä½“ç®¡è§„æ¨¡çš„å¢é•¿ï¼Œé€»è¾‘æ ¸å¿ƒçš„æ•°é‡è¿…é€Ÿå¢åŠ ã€‚


é‡ç‚¹ï¼šå¤šæ ¸æ¶æ„ï¼Œé€šè¿‡å¢åŠ æ•°é‡
![image.png](https://s2.loli.net/2025/05/30/WvsntA6r7kH2a4Q.png)

It has been challenging for Ceph performance to keep up with the pace of modern hardware development, as its architecture was shaped more than a decade ago â€“

its dependence on single core performance **limits** its ability to take full advantage of the growing capabilities of IO devices.

Ceph æ€§èƒ½è¦è·Ÿä¸Šç°ä»£ç¡¬ä»¶å‘å±•çš„æ­¥ä¼ä¸€ç›´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒçš„æ¶æ„æ˜¯åœ¨åå¤šå¹´å‰å½¢æˆçš„ï¼Œå®ƒå¯¹å•æ ¸æ€§èƒ½çš„ä¾èµ–é™åˆ¶äº†å®ƒå……åˆ†åˆ©ç”¨ IO è®¾å¤‡ä¸æ–­å¢é•¿çš„åŠŸèƒ½çš„èƒ½åŠ›
### ç°ä»£ IO å’Œ CPU çš„æ¼”è¿›è¶‹åŠ¿ï¼š

| èµ„æºç±»å‹ | æ¼”è¿›æ–¹å‘                 | å¯¹ Ceph æ¶æ„çš„æŒ‘æˆ˜     |
| ---- | -------------------- | ---------------- |
| CPU  | å¤šæ ¸ > å•æ ¸              | ä¸²è¡Œç“¶é¢ˆé™åˆ¶å¹¶è¡Œè®¡ç®—èƒ½åŠ›     |
| å­˜å‚¨   | NVMeã€SSD             | Ceph æ— æ³•äº§ç”Ÿè¶³å¤Ÿå¹¶è¡Œå†™è¯·æ±‚ |
| ç½‘ç»œ   | RDMAã€é«˜å¸¦å®½ç½‘å¡ï¼ˆ25G/100Gï¼‰ | å•çº¿ç¨‹ç½‘ç»œå¤„ç†æ’‘ä¸èµ·ååé‡    |

In particular, classic Ceph object storage daemon (OSD)â€™s reliance on thread pools to handle different portions of an IO incur**s significant latency costs due to cross-core communications.** 
Reducing or eliminating those costs is the core goal of the Crimson project.  

ç‰¹åˆ«æ˜¯ï¼Œç”±äºè·¨æ ¸å¿ƒé€šä¿¡ï¼Œä¼ ç»Ÿ Ceph å¯¹è±¡å­˜å‚¨å®ˆæŠ¤è¿›ç¨‹ ï¼ˆOSDï¼‰ ä¾èµ–çº¿ç¨‹æ± æ¥å¤„ç† IO çš„ä¸åŒéƒ¨åˆ†ï¼Œè¿™ä¼šäº§ç”Ÿå·¨å¤§çš„å»¶è¿Ÿæˆæœ¬ã€‚å‡å°‘æˆ–æ¶ˆé™¤è¿™äº›æˆæœ¬æ˜¯ Crimson é¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡ã€‚

ç–‘é—®ï¼šæ€ä¹ˆè§£å†³å¤šæ ¸æ¶æ„äº¤äº’é—®é¢˜ï¼Ÿ

The Crimson project was initiated t**o rewrite the Ceph OSD with the shared-nothing desig**n and run-to-completion model to address the demanding scaling requirements, and at the same time, be compatible with the existing clients and components.  

Crimson é¡¹ç›®æ—¨åœ¨ä½¿ç”¨æ— å…±äº«è®¾è®¡å’Œè¿è¡Œåˆ°å®Œæˆæ¨¡å‹é‡å†™ Ceph OSDï¼Œä»¥æ»¡è¶³è‹›åˆ»çš„æ‰©å±•è¦æ±‚ï¼ŒåŒæ—¶ä¸ç°æœ‰å®¢æˆ·ç«¯å’Œç»„ä»¶å…¼å®¹ã€‚
### **Seastar æ¡†æ¶ï¼šé«˜æ€§èƒ½äº‹ä»¶é©±åŠ¨ç¼–ç¨‹æ¨¡å‹**

- Crimson åŸºäº **[Seastar](https://github.com/scylladb/seastar)** æ¡†æ¶æ„å»ºï¼Œè¿™æ˜¯ä¸€ä¸ªä¸º**é«˜æ€§èƒ½æœåŠ¡å™¨åº”ç”¨è®¾è®¡çš„ C++ å¼‚æ­¥ç¼–ç¨‹æ¡†æ¶**ã€‚
    
- ç‰¹ç‚¹ï¼š
    - æ¯ä¸ªæ ¸å¿ƒè¿è¡Œç‹¬ç«‹çš„äº‹ä»¶å¾ªç¯ï¼ˆreactorï¼‰
    - ä½¿ç”¨ **`future/promise`** è¿›è¡Œå¼‚æ­¥ä»»åŠ¡ç¼–æ’ï¼ˆæ— å›è°ƒï¼‰
    - æ— éœ€çº¿ç¨‹é—´å…±äº«å†…å­˜ï¼Œ**æ¯ä¸ª core æ‹¥æœ‰è‡ªå·±çš„æ•°æ®ä¸é€»è¾‘**
    - å®Œå…¨ç”¨æˆ·æ€ I/Oï¼ˆbypass å†…æ ¸ context switchï¼‰


Crimson ç›¸è¾ƒä¼ ç»Ÿ OSD çš„åŒºåˆ«

| ç‰¹æ€§     | ä¼ ç»Ÿ Ceph OSD    | Crimson OSD (Seastar/æ— å…±äº«) |
| ------ | -------------- | ------------------------- |
| å¤šæ ¸åˆ©ç”¨æ–¹å¼ | å¤šçº¿ç¨‹ + é”        | æ¯æ ¸ç‹¬ç«‹äº‹ä»¶å¾ªç¯ï¼Œæ— é”               |
| çŠ¶æ€å…±äº«   | å…¨å±€çŠ¶æ€ã€å…±äº« PGã€ç¼“å­˜ç­‰ | æ¯æ ¸ç‹¬å çŠ¶æ€ï¼ŒPG åˆ†åŒº              |
| å¹¶å‘å¤„ç†æ¨¡å‹ | å¤šçº¿ç¨‹äº’æ–¥è°ƒåº¦        | å¼‚æ­¥äº‹ä»¶ + run-to-completion  |
| é€šä¿¡æ–¹å¼   | å…±äº«å†…å­˜æˆ–å…¨å±€é˜Ÿåˆ—      | è·¨æ ¸æ¶ˆæ¯ä¼ é€’                    |
| è°ƒåº¦ç²’åº¦   | æ“ä½œç³»ç»Ÿçº¿ç¨‹è°ƒåº¦       | åº”ç”¨å±‚è½»é‡åç¨‹è°ƒåº¦                 |

To understand how Crimson OSD is **re-designed for CPU scaling**, 
we compare the architectural differences between Classic and Crimson OSD 
to explain how and why the design has been altered. 

ä¸ºäº†äº†è§£ Crimson OSD å¦‚ä½•é’ˆå¯¹ CPU æ‰©å±•è¿›è¡Œé‡æ–°è®¾è®¡ï¼Œ
æˆ‘ä»¬æ¯”è¾ƒäº† Classic å’Œ Crimson OSD ä¹‹é—´çš„æ¶æ„å·®å¼‚ï¼Œ
ä»¥è§£é‡Šè®¾è®¡æ˜¯å¦‚ä½•ä»¥åŠä¸ºä»€ä¹ˆè¢«æ›´æ”¹çš„


Then we discuss why Crimson builds on the **Seastar framework**, 
and how each major component is implemented for scaling.
ç„¶åï¼Œæˆ‘ä»¬è®¨è®ºä¸ºä»€ä¹ˆ Crimson åŸºäº Seastar æ¡†æ¶æ„å»ºï¼Œ
ä»¥åŠå¦‚ä½•å®ç°æ¯ä¸ªä¸»è¦ç»„ä»¶ä»¥è¿›è¡Œæ‰©å±•

é‡ç‚¹ï¼šä¸æ˜¯è‡ªå·±å‘æ˜è½®å­ï¼Œç”¨**Seastar framework** é‡æ–°è®¾è®¡è‡ªå·±ä¸šåŠ¡

Finally, we share the latest status towards this goal,
and with meaningful performance results
that might be helpful to illustrate the final vision.  

æœ€åï¼Œæˆ‘ä»¬åˆ†äº«äº†å®ç°è¿™ä¸€ç›®æ ‡çš„æœ€æ–°çŠ¶æ€ï¼Œä»¥åŠæœ‰æ„ä¹‰çš„æ€§èƒ½ç»“æœï¼Œ
è¿™äº›ç»“æœå¯èƒ½æœ‰åŠ©äºè¯´æ˜æœ€ç»ˆæ„¿æ™¯ã€‚

å•è¯ï¼š
â€‹æœ€ç»ˆæ„¿æ™¯â€‹**â€‹ â†’ Final vision

å…³é”®æ€§èƒ½ç»“æœâ€‹**â€‹ â†’ Meaningful performance results
# Crimson vs Classic OSD Architecture  

Crimson vs Classic OSD æ¶æ„

ç–‘é—®ï¼šæ¶æ„æ˜¯ä»€ä¹ˆï¼Œè§£å†³ä»€ä¹ˆé—®é¢˜,ä¸ºä»€ä¹ˆé‡æ„


Ceph OSD is a part of Ceph cluster responsible for providing object access over the **network**, maintaining **redundancy** and high availability and persisting objects to a local storage device. 

Ceph OSD æ˜¯ Ceph é›†ç¾¤çš„ä¸€éƒ¨åˆ†ï¼Œè´Ÿè´£é€šè¿‡ç½‘ç»œæä¾›å¯¹è±¡è®¿é—®ã€ç»´æŠ¤å†—ä½™å’Œé«˜å¯ç”¨æ€§ï¼Œä»¥åŠå°†å¯¹è±¡æŒä¹…ä¿å­˜åˆ°æœ¬åœ°å­˜å‚¨è®¾å¤‡




Ceph OSD **modularity** such as Messenger, OSD services, and ObjectStore are unchanged in their **responsibilities**, 
Ceph OSD æ¨¡å—åŒ–ï¼ˆå¦‚ Messengerã€OSD æœåŠ¡å’Œ ObjectStoreï¼‰çš„èŒè´£ä¿æŒä¸å˜

### 1. **OSD æ¨¡å—èŒè´£ä¸å˜**

- **OSDï¼ˆObject Storage Daemonï¼‰** æ˜¯ Ceph ä¸­çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£æ•°æ®çš„å­˜å‚¨ã€å¤åˆ¶ã€æ¢å¤å’Œå†å¹³è¡¡ã€‚
    
- å…¶ä¸­çš„å­æ¨¡å—å¦‚ï¼š
    
    - **Messenger**ï¼šè´Ÿè´£ç½‘ç»œé€šä¿¡åè®®å¤„ç†ï¼›
        
    - **OSD services**ï¼šæ‰§è¡Œå¦‚ peeringã€replication ç­‰æ ¸å¿ƒé€»è¾‘ï¼›
        
    - **ObjectStore**ï¼šæä¾›ä¸åç«¯å­˜å‚¨ï¼ˆå¦‚ BlueStoreï¼‰çš„å…·ä½“è¯»å†™æ¥å£ã€‚
        

è™½ç„¶è¿›è¡Œäº†åº•å±‚æ¶æ„ä¼˜åŒ–ï¼Œä½†è¿™äº›æ¨¡å—"**è¯¥åšä»€ä¹ˆ**"çš„èŒè´£ï¼ˆresponsibilitiesï¼‰**æ²¡å˜**ã€‚


but the form of cross-component interactions and internal resource management are vastly refactored to apply the **shared-nothing design** and user-space task scheduling bottom up.  
ä½†è·¨ç»„ä»¶äº¤äº’å’Œå†…éƒ¨èµ„æºç®¡ç†çš„å½¢å¼è¢«å¤§å¤§é‡æ„ï¼Œ
ä»¥è‡ªä¸‹è€Œä¸Šåœ°åº”ç”¨æ— å…±äº«è®¾è®¡å’Œç”¨æˆ·ç©ºé—´ä»»åŠ¡è°ƒåº¦ã€‚

### **è·¨ç»„ä»¶äº¤äº’å’Œèµ„æºç®¡ç†é‡æ„**

- è¿‡å»ï¼ŒCeph å†…éƒ¨æ¨¡å—ä¹‹é—´çš„äº¤äº’ä»¥åŠèµ„æºè°ƒåº¦å¤šä¾èµ–äºå†…æ ¸çº¿ç¨‹ã€å…¨å±€é”ã€å…±äº«çŠ¶æ€ç­‰æœºåˆ¶ï¼Œå®¹æ˜“å¼•èµ·æ€§èƒ½ç“¶é¢ˆå’Œèµ„æºç«äº‰ã€‚
    
- ç°åœ¨ Ceph é‡æ„äº†è¿™éƒ¨åˆ†ï¼šå®ç°äº†æ›´ä¸ºæ¸…æ™°å’Œç‹¬ç«‹çš„æ¨¡å—è¾¹ç•Œï¼Œé‡‡ç”¨ç°ä»£åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„ç†å¿µï¼Œå¦‚ï¼š
    

#### ğŸ”¹ **Shared-nothing designï¼ˆå…±äº«æ— è®¾è®¡ï¼‰**

- æ¯ä¸ªç»„ä»¶å°½å¯èƒ½ç‹¬ç«‹è¿è¡Œï¼Œä¸å…±äº«å†…å­˜ã€ä¸å…±äº«é”ã€‚
    
- ä¼˜ç‚¹ï¼š
    
    - é™ä½èµ„æºäº‰ç”¨ï¼›
        
    - æ›´å¥½åœ°å¹¶è¡Œæ‰©å±•ï¼›
        
    - å®¹é”™æ€§å’Œéš”ç¦»æ€§æ›´å¼ºã€‚
        

ä¾‹å¦‚ï¼Œå¤šä¸ª OSD çº¿ç¨‹ä¸ä¼šå…±äº«çŠ¶æ€å˜é‡ï¼Œè€Œæ˜¯é€šè¿‡æ¶ˆæ¯ä¼ é€’ï¼ˆmessage passingï¼‰æ¥é€šä¿¡ã€‚

#### ğŸ”¹ **User-space task schedulingï¼ˆç”¨æˆ·æ€ä»»åŠ¡è°ƒåº¦ï¼‰**

- Ceph åº•å±‚ä¸å†ä¾èµ–æ“ä½œç³»ç»Ÿå†…æ ¸çš„è°ƒåº¦å™¨ï¼ˆå¦‚ POSIX threadsï¼‰ï¼Œè€Œæ˜¯å°†è°ƒåº¦é€»è¾‘è¿ç§»åˆ°äº†ç”¨æˆ·ç©ºé—´ã€‚
    
- å¸¸ç”¨æŠ€æœ¯åŒ…æ‹¬ï¼š
    
    - åç¨‹ï¼ˆcoroutines / fibersï¼‰ï¼›
        
    - äº‹ä»¶é©±åŠ¨æ¡†æ¶ï¼ˆå¦‚ Seastar, io_uring ç­‰ï¼‰ï¼›
        
    - è‡ªå®šä¹‰è°ƒåº¦å™¨ï¼ˆä¾‹å¦‚åœ¨ BlueStore/Crimson ä¸­å®ç°ï¼‰ã€‚

ç¡®å®ï¼Œâ€‹**â€‹æ˜¾å¼é€šä¿¡â€‹**â€‹ï¼ˆå¦‚ç½‘ç»œæ¶ˆæ¯ã€IPCï¼‰ç›¸æ¯”â€‹**â€‹å…±äº«å†…å­˜â€‹**â€‹é€šå¸¸ä¼šæœ‰æ›´é«˜çš„å»¶è¿Ÿå’Œå¼€é”€ï¼Œ
ä½†ä¸ºä»€ä¹ˆç°ä»£ç³»ç»Ÿï¼ˆå¦‚å¾®æœåŠ¡ã€åˆ†å¸ƒå¼æ•°æ®åº“ï¼‰ä»å¹¿æ³›é‡‡ç”¨è¿™ç§è®¾è®¡ï¼Ÿå…³é”®åœ¨äºâ€‹**â€‹æƒè¡¡å–èˆâ€‹**â€‹å’Œâ€‹**â€‹å·¥ç¨‹ä¼˜åŒ–â€‹**â€‹ã€‚ä»¥ä¸‹æ˜¯é€å±‚è§£æï¼š

---

### â€‹**â€‹1. æ˜¾å¼é€šä¿¡ vs. å…±äº«å†…å­˜çš„æ€§èƒ½å¯¹æ¯”â€‹**â€‹

|â€‹**â€‹ç»´åº¦â€‹**â€‹|â€‹**â€‹å…±äº«å†…å­˜â€‹**â€‹|â€‹**â€‹æ˜¾å¼é€šä¿¡ï¼ˆç½‘ç»œ/IPCï¼‰â€‹**â€‹|
|---|---|---|
|â€‹**â€‹å»¶è¿Ÿâ€‹**â€‹|æä½ï¼ˆçº³ç§’çº§ï¼Œå¦‚CPUç¼“å­˜è®¿é—®ï¼‰|è¾ƒé«˜ï¼ˆå¾®ç§’~æ¯«ç§’çº§ï¼Œä¾èµ–ä¼ è¾“ä»‹è´¨ï¼‰|
|â€‹**â€‹ååâ€‹**â€‹|é«˜ï¼ˆå†…å­˜å¸¦å®½å¯è¾¾GB/sï¼‰|å—ç½‘ç»œå¸¦å®½é™åˆ¶ï¼ˆå¦‚10Gbpsâ‰ˆ1.25GB/sï¼‰|
|â€‹**â€‹æ‰©å±•æ€§â€‹**â€‹|å·®ï¼ˆé”ç«äº‰éšç»„ä»¶æ•°å¢åŠ è€ŒåŠ å‰§ï¼‰|å¥½ï¼ˆç»„ä»¶å¯åˆ†å¸ƒå¼éƒ¨ç½²ï¼‰|
|â€‹**â€‹å®¹é”™æ€§â€‹**â€‹|å·®ï¼ˆå•ç‚¹æ•…éšœæ˜“æ‰©æ•£ï¼‰|å¥½ï¼ˆæ•…éšœéš”ç¦»æ€§å¼ºï¼‰|
|â€‹**â€‹ç¼–ç¨‹å¤æ‚åº¦â€‹**â€‹|é«˜ï¼ˆéœ€å¤„ç†ç«æ€ã€æ­»é”ï¼‰|ä½ï¼ˆæ¶ˆæ¯å¼‚æ­¥å¤„ç†ï¼Œé€»è¾‘è§£è€¦ï¼‰|

---

### â€‹**â€‹2. ä¸ºä»€ä¹ˆæ˜¾å¼é€šä¿¡ä»è¢«é‡‡ç”¨ï¼Ÿâ€‹**â€‹

#### ï¼ˆ1ï¼‰â€‹**â€‹æ‰©å±•æ€§éœ€æ±‚å‹å€’æ€§èƒ½æŸè€—â€‹**â€‹

- â€‹**â€‹å…±äº«å†…å­˜çš„ç“¶é¢ˆâ€‹**â€‹ï¼šå½“ç»„ä»¶æ•°é‡å¢åŠ æ—¶ï¼Œé”ç«äº‰ã€ç¼“å­˜ä¸€è‡´æ€§ï¼ˆCache Coherencyï¼‰ä¼šå¯¼è‡´æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼ˆå¦‚Amdahlå®šå¾‹ï¼‰ã€‚
- â€‹**â€‹æ˜¾å¼é€šä¿¡çš„ä¼˜åŠ¿â€‹**â€‹ï¼šé€šè¿‡æ°´å¹³æ‰©å±•ï¼ˆåŠ æœºå™¨ï¼‰çº¿æ€§æå‡æ€§èƒ½ï¼Œé€‚åˆäº‘è®¡ç®—ã€åˆ†å¸ƒå¼åœºæ™¯ã€‚

#### ï¼ˆ2ï¼‰â€‹**â€‹ç°ä»£ç¡¬ä»¶/åè®®ä¼˜åŒ–æ˜¾å¼é€šä¿¡â€‹**â€‹

- â€‹**â€‹RDMAï¼ˆè¿œç¨‹ç›´æ¥å†…å­˜è®¿é—®ï¼‰â€‹**â€‹ï¼šç»•è¿‡OSå†…æ ¸ï¼Œå»¶è¿Ÿå¯ä½è‡³1Î¼sï¼ˆå¦‚InfiniBandï¼‰ã€‚
- â€‹**â€‹é›¶æ‹·è´æŠ€æœ¯â€‹**â€‹ï¼šå¦‚Linuxçš„`io_uring`ã€DPDKï¼Œå‡å°‘æ•°æ®å¤åˆ¶å¼€é”€ã€‚
- â€‹**â€‹é«˜æ•ˆåºåˆ—åŒ–â€‹**â€‹ï¼šProtobufã€FlatBufferæ¯”JSON/XMLå¿«10~100å€ã€‚

#### ï¼ˆ3ï¼‰â€‹**â€‹å¼‚æ­¥åŒ–ä¸æ‰¹å¤„ç†æŠµæ¶ˆå»¶è¿Ÿâ€‹**â€‹

- â€‹**â€‹æ¶ˆæ¯æ‰¹å¤„ç†â€‹**â€‹ï¼šå¦‚Kafkaçš„Produceræ‰¹é‡å‘é€ã€‚
- â€‹**â€‹æµæ°´çº¿åŒ–â€‹**â€‹ï¼šé‡å é€šä¿¡ä¸è®¡ç®—ï¼ˆå¦‚GPU-CPUå¼‚æ„è®¡ç®—ï¼‰ã€‚
- â€‹**â€‹åç¨‹/äº‹ä»¶é©±åŠ¨â€‹**â€‹ï¼šé¿å…çº¿ç¨‹é˜»å¡ï¼ˆå¦‚Goçš„Goroutineï¼‰
#### 1ï¼‰â€‹**â€‹Shared-Nothing Designï¼ˆæ— å…±äº«è®¾è®¡ï¼‰â€‹**â€‹

- â€‹**â€‹æœ¬è´¨â€‹**â€‹ï¼šç³»ç»Ÿç»„ä»¶ï¼ˆå¦‚è¿›ç¨‹ã€çº¿ç¨‹ã€èŠ‚ç‚¹ï¼‰â€‹**â€‹ä¸å…±äº«å†…å­˜æˆ–ç£ç›˜å­˜å‚¨â€‹**â€‹ï¼Œä»…é€šè¿‡â€‹**â€‹æ˜¾å¼é€šä¿¡â€‹**â€‹ï¼ˆå¦‚ç½‘ç»œæ¶ˆæ¯ã€IPCï¼‰äº¤äº’ã€‚
- â€‹**â€‹è®¡ç®—æœºä¸­çš„å…¸å‹åº”ç”¨â€‹**â€‹ï¼š
    - â€‹**â€‹åˆ†å¸ƒå¼ç³»ç»Ÿâ€‹**â€‹ï¼šå¦‚Kubernetes Podã€å¾®æœåŠ¡æ¶æ„ï¼ˆæ¯ä¸ªæœåŠ¡ç‹¬ç«‹èµ„æºï¼‰ã€‚
    - â€‹**â€‹æ•°æ®åº“åˆ†ç‰‡â€‹**â€‹ï¼šå¦‚MySQLåˆ†åº“åˆ†è¡¨ï¼Œæ¯ä¸ªåˆ†ç‰‡ç‹¬å èµ„æºã€‚
    - â€‹**â€‹é«˜æ€§èƒ½è®¡ç®—â€‹**â€‹ï¼šMPIç¼–ç¨‹ä¸­è¿›ç¨‹é—´é€šè¿‡æ¶ˆæ¯ä¼ é€’é€šä¿¡ã€‚
- â€‹**â€‹ä¼˜åŠ¿â€‹**â€‹ï¼š
    - â€‹**â€‹å¯æ‰©å±•æ€§â€‹**â€‹ï¼šæ— å…±äº«çŠ¶æ€=æ— é”ç«äº‰ï¼Œæ˜“æ°´å¹³æ‰©å±•ã€‚
    - â€‹**â€‹å®¹é”™æ€§â€‹**â€‹ï¼šå•ç»„ä»¶æ•…éšœä¸å½±å“å…¶ä»–ç»„ä»¶ã€‚
    - â€‹**â€‹ç¡®å®šæ€§â€‹**â€‹ï¼šé¿å…å…±äº«å†…å­˜çš„ç«æ€æ¡ä»¶ï¼ˆRace Conditionï¼‰ã€‚

#### ï¼ˆ2ï¼‰â€‹**â€‹User-Space Task Schedulingï¼ˆç”¨æˆ·æ€ä»»åŠ¡è°ƒåº¦ï¼‰â€‹**â€‹

- â€‹**â€‹ä¸ä¼ ç»Ÿå†…æ ¸è°ƒåº¦çš„åŒºåˆ«â€‹**â€‹ï¼š
    - â€‹**â€‹å†…æ ¸è°ƒåº¦â€‹**â€‹ï¼šç”±OSå†…æ ¸ç»Ÿä¸€ç®¡ç†çº¿ç¨‹/è¿›ç¨‹ï¼Œæ¶‰åŠç‰¹æƒçº§åˆ‡æ¢ï¼ˆä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€é«˜ï¼‰ã€‚
    - â€‹**â€‹ç”¨æˆ·æ€è°ƒåº¦â€‹**â€‹ï¼šåº”ç”¨è‡ªè¡Œç®¡ç†ä»»åŠ¡ï¼ˆå¦‚åç¨‹ã€è½»é‡çº§çº¿ç¨‹ï¼‰ï¼Œâ€‹**â€‹æ— éœ€é™·å…¥å†…æ ¸â€‹**â€‹ã€‚
- â€‹**â€‹å…¸å‹å®ç°â€‹**â€‹ï¼š
    - â€‹**â€‹åç¨‹åº“â€‹**â€‹ï¼šå¦‚Goçš„Goroutineã€Rustçš„Tokioã€‚
    - â€‹**â€‹DPDK/SPDKâ€‹**â€‹ï¼šç»•è¿‡å†…æ ¸ç›´æ¥ç®¡ç†ç½‘å¡/ç£ç›˜ã€‚

SPDKå’ŒDPDKçš„å…³ç³»
SPDKï¼ˆStorage Performance Development Kitï¼‰å’ŒDPDKï¼ˆData Plane Development Kitï¼‰æ˜¯ä¸¤ä¸ªç”±Intelå¼€æºçš„é¡¹ç›®ï¼Œç”¨äºä¼˜åŒ–å­˜å‚¨å’Œç½‘ç»œæ•°æ®å¹³é¢çš„æ€§èƒ½ã€‚

å°½ç®¡SPDKå’ŒDPDKæ˜¯ç‹¬ç«‹çš„é¡¹ç›®ï¼Œä½†å®ƒä»¬ä¹‹é—´å­˜åœ¨ç´§å¯†çš„å…³ç³»å’Œäº’è¡¥çš„ä½œç”¨ã€‚ä¸‹é¢æ˜¯å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼š

å…±åŒçš„ç›®æ ‡ï¼šSPDKå’ŒDPDKçš„å…±åŒç›®æ ‡æ˜¯é€šè¿‡æä¾›é«˜æ€§èƒ½çš„è½¯ä»¶åº“å’Œå·¥å…·ï¼ŒåŠ é€Ÿå­˜å‚¨å’Œç½‘ç»œæ•°æ®å¹³é¢çš„å¼€å‘å’Œéƒ¨ç½²ã€‚

å…±äº«çš„ç»„ä»¶ï¼šSPDKå’ŒDPDKåœ¨æŸäº›æ–¹é¢å…±äº«ç›¸åŒçš„ç»„ä»¶å’ŒæŠ€æœ¯ã€‚ä¾‹å¦‚ï¼Œå®ƒä»¬éƒ½ä½¿ç”¨äº†User 

    



The architecture of the Classic OSD isn't friendly to **multiple cores with worker thread pools** in each component and **shared queue**s among them.

Classic OSD çš„æ¶æ„å¯¹å¤šä¸ªå†…æ ¸ä¸å‹å¥½ï¼Œæ¯ä¸ªç»„ä»¶ä¸­éƒ½æœ‰ worker çº¿ç¨‹æ± ï¼Œå¹¶ä¸”å®ƒä»¬ä¹‹é—´æœ‰å…±äº«é˜Ÿåˆ—
![å•æ ¸å¹¶å‘å¤„ç†](https://s2.loli.net/2025/05/30/DJ73KOEqGTgWPVb.png)


In a simplified example, a PG operation requires to be firstly processed by a **messenger worker threa**d to assemble or decode raw data stream into a message, then put into a message queue for dispatching. 
åœ¨ç®€åŒ–çš„ä¾‹å­ä¸­ï¼ŒPG ä½œéœ€è¦é¦–å…ˆç”± Messenger å·¥ä½œçº¿ç¨‹å¤„ç†ï¼Œå°†åŸå§‹æ•°æ®æµç»„è£…æˆ–è§£ç ä¸ºæ¶ˆæ¯ï¼Œç„¶åæ”¾å…¥æ¶ˆæ¯é˜Ÿåˆ—è¿›è¡Œè°ƒåº¦

Later, **a PG worker thread** picks up the message, after necessary processing, **hands over** the request to the ObjectStore in the form of transactions. 

ç¨åï¼Œä¸€ä¸ª PG worker çº¿ç¨‹è·å–æ¶ˆæ¯ï¼Œç»è¿‡å¿…è¦çš„å¤„ç†åï¼Œå°†è¯·æ±‚ä»¥äº‹åŠ¡çš„å½¢å¼äº¤ç»™ ObjectStore

After the transaction is committed, the PG will complete the operation and send the reply through the sending queue and the messenger worker thread again.
äº‹åŠ¡æäº¤åï¼ŒPG å°†å®Œæˆä½œï¼Œå¹¶å†æ¬¡é€šè¿‡å‘é€é˜Ÿåˆ—å’Œ Messenger å·¥ä½œçº¿ç¨‹å‘é€å›å¤

Although the workload is allowed to **scale to multiple CPUs** by adding more threads into the pool, these threads are by default sharing the resources and thus require **locks** which introduce contentions. 

äº‹åŠ¡æäº¤åï¼ŒPG å°†å®Œæˆä½œï¼Œå¹¶å†æ¬¡é€šè¿‡å‘é€é˜Ÿåˆ—å’Œ Messenger å·¥ä½œçº¿ç¨‹å‘é€å›å¤

And the **reality** is more **complicated** as further thread pools are implemented inside each component, and the **data path** is longer if there are replications across the OSDs.  

éšç€æ¯ä¸ªç»„ä»¶å†…éƒ¨å®æ–½äº†æ›´å¤šçš„çº¿ç¨‹æ± ï¼Œç°å®æƒ…å†µæ›´åŠ å¤æ‚ï¼Œ
å¦‚æœè·¨ OSD å­˜åœ¨å¤åˆ¶ï¼Œåˆ™æ•°æ®è·¯å¾„ä¼šæ›´é•¿ã€‚


**A major challenge** of the Classic architecture is that lock contentionï¼ˆé”äº‰ç”¨ï¼‰ **overhead** scales rapidly with the number of tasks and cores, and every locking point may become the **scaling bottleneck** under certain scenarios. 

Classic æ¶æ„çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯é”äº‰ç”¨å¼€é”€ä¼šéšç€ä»»åŠ¡å’Œå†…æ ¸çš„æ•°é‡è€Œå¿«é€Ÿæ‰©å±•ï¼Œ
å¹¶ä¸”æ¯ä¸ªé”å®šç‚¹åœ¨æŸäº›æƒ…å†µä¸‹éƒ½å¯èƒ½æˆä¸ºæ‰©å±•ç“¶é¢ˆ


**Moreover**, these locks and **queues**(é˜Ÿåˆ—) **incur** **latency** costs even when uncontended(æ²¡æœ‰äº‰ç”¨).

æ­¤å¤–ï¼Œå³ä½¿æ²¡æœ‰äº‰ç”¨ï¼Œè¿™äº›é”å’Œé˜Ÿåˆ—ä¹Ÿä¼šäº§ç”Ÿå»¶è¿Ÿæˆæœ¬


There have been considerable(ç›¸å½“å¤§) efforts over the years on **profiling** and optimizations for finer-grained(æ›´ç»†ç²’åº¦) resource management and **fast path implementations to skip queueing**.

å¤šå¹´æ¥ï¼Œäººä»¬åœ¨åˆ†æå’Œä¼˜åŒ–æ›´ç»†ç²’åº¦çš„èµ„æºç®¡ç†å’Œè·³è¿‡æ’é˜Ÿçš„å¿«é€Ÿè·¯å¾„å®ç°æ–¹é¢
ä»˜å‡ºäº†ç›¸å½“å¤§çš„åŠªåŠ›

å•è¯ï¼šâ€‹**â€‹
Finer-grained Resource Managementï¼ˆç»†ç²’åº¦èµ„æºç®¡ç†ï¼‰â€‹
- â€‹**â€‹ç›®æ ‡â€‹**â€‹ï¼šå°†èµ„æºï¼ˆCPUã€å†…å­˜ã€I/Oï¼‰çš„åˆ†é…å•ä½ä»"ç²—ç²’åº¦"ï¼ˆå¦‚è¿›ç¨‹ã€çº¿ç¨‹ï¼‰ç»†åŒ–åˆ°"å¾®ç²’åº¦"ï¼ˆå¦‚åç¨‹ã€å†…å­˜å—ã€NUMAèŠ‚ç‚¹ï¼‰ã€‚
- **Fast Path Implementationsï¼ˆå¿«é€Ÿè·¯å¾„å®ç°ï¼‰â€‹**â€‹
    - â€‹**â€‹æœ¬è´¨â€‹**â€‹ï¼šåœ¨é€šç”¨å¤„ç†æµç¨‹ä¸­è¯†åˆ«é«˜é¢‘/ä½å»¶è¿Ÿè·¯å¾„ï¼Œé€šè¿‡â€‹**â€‹ç»•è¿‡é˜Ÿåˆ—/é”/ä¸­é—´å±‚â€‹**â€‹ç›´æ¥å¤„ç†ã€‚
    - â€‹**â€‹æ¡ˆä¾‹â€‹**â€‹ï¼š
        - â€‹**â€‹ç½‘ç»œåè®®æ ˆâ€‹**â€‹ï¼šDPDK/XDPç»•è¿‡å†…æ ¸åè®®æ ˆï¼ˆèŠ‚çœÎ¼sçº§å»¶è¿Ÿï¼‰ã€‚
        - â€‹**â€‹å­˜å‚¨æ ˆâ€‹**â€‹ï¼šNVMe over Fabricsçš„`Zero-Copy RDMA`ã€‚
        - â€‹**â€‹æ•°æ®åº“â€‹**â€‹ï¼šLSM-Treeçš„MemTableç›´æ¥å†™å…¥ï¼ˆè·³è¿‡WALé˜Ÿåˆ—ï¼‰ã€‚
- â€‹**â€‹Skipping Queueingï¼ˆè·³è¿‡æ’é˜Ÿï¼‰â€‹**â€‹
    - â€‹**â€‹ä¸ºä»€ä¹ˆéœ€è¦è·³è¿‡ï¼Ÿâ€‹**â€‹  
        é˜Ÿåˆ—å¼•å…¥çš„å»¶è¿ŸåŒ…æ‹¬ï¼šâ€‹**â€‹å…¥é˜Ÿ/å‡ºé˜Ÿæ“ä½œâ€‹**â€‹ã€â€‹**â€‹ç­‰å¾…è°ƒåº¦â€‹**â€‹ã€â€‹**â€‹ä¸Šä¸‹æ–‡åˆ‡æ¢â€‹**â€‹ã€‚
    - â€‹**â€‹å¦‚ä½•è·³è¿‡ï¼Ÿâ€‹**â€‹
        - â€‹**â€‹ä¼˜å…ˆçº§æŠ¢å â€‹**â€‹ï¼šå®æ—¶ä»»åŠ¡ç›´æ¥æŠ¢å CPUï¼ˆå¦‚Linux RT-Preemptï¼‰ã€‚
        - â€‹**â€‹æ— é”è®¾è®¡â€‹**â€‹ï¼šå¦‚Seastaræ¡†æ¶çš„Shard-per-Coreæ¶æ„ã€‚
#### æ¡ˆä¾‹ï¼šåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿï¼ˆå¦‚Ceph Bluestoreï¼‰

| â€‹**â€‹ä¼˜åŒ–ç›®æ ‡â€‹**â€‹  | â€‹**â€‹ç»†ç²’åº¦èµ„æºç®¡ç†â€‹**â€‹          | â€‹**â€‹å¿«é€Ÿè·¯å¾„å®ç°â€‹**â€‹         | â€‹**â€‹ååŒæ•ˆåº”â€‹**â€‹            |
| ------------- | ------------------------ | ---------------------- | ----------------------- |
| â€‹**â€‹å†™è¯·æ±‚å¤„ç†â€‹**â€‹ | æ¯ä¸ªNVMe SSDç»‘å®šç‹¬ç«‹CPUæ ¸å¿ƒ      | ç›´æ¥è°ƒç”¨SPDKè½®è¯¢é©±åŠ¨ï¼Œè·³è¿‡å†…æ ¸é˜Ÿåˆ—    | å•è®¾å¤‡ååè¾¾ â€‹**â€‹1M+ IOPSâ€‹**â€‹ |
| â€‹**â€‹å…ƒæ•°æ®æ“ä½œâ€‹**â€‹ | å…ƒæ•°æ®åˆ†åŒºç‹¬å NUMAèŠ‚ç‚¹å†…å­˜          | æ— é”è·³è¡¨ï¼ˆSkipListï¼‰æ›¿ä»£B-Tree | æŸ¥è¯¢å»¶è¿Ÿé™ä½ â€‹**â€‹10å€â€‹**â€‹      |
| â€‹**â€‹æ•°æ®æ ¡éªŒâ€‹**â€‹  | CRCè®¡ç®—ä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿå™¨ï¼ˆå¦‚Intel QATï¼‰ | æ ¡éªŒå¤±è´¥ç›´æ¥ä¸¢å¼ƒï¼Œä¸è¿›é€šç”¨è·¯å¾„        | æ— æ•ˆæ•°æ®å¤„ç†å¼€é”€ â€‹**â€‹**         |

There will be less low-hanging fruits in the future and the scalability seems to be converging into a certain multiplicator ï¼Œ
æœªæ¥å”¾æ‰‹å¯å¾—çš„æœå®ä¼šæ›´å°‘

1. â€‹**â€‹Low-hanging fruitsï¼ˆå”¾æ‰‹å¯å¾—çš„æœºä¼šï¼‰â€‹**â€‹  
    â†’ æŒ‡ä¸éœ€è¦å¤æ‚åŠªåŠ›å³å¯å®ç°çš„æ€§èƒ½ä¼˜åŒ–
2. â€‹**â€‹Scalability convergingï¼ˆå¯æ‰©å±•æ€§è¶‹äºæ”¶æ•›ï¼‰â€‹**â€‹  
    â†’ æš—å–»æ€§èƒ½éšèµ„æºå¢åŠ çš„æ”¶ç›Šé€’å‡ï¼ˆçªç ´éœ€æ¶æ„é©æ–°ï¼‰  
    _ä¾‹ï¼šåƒæ ¸æ‰©å±•ä»çº¿æ€§(1â†’N)é€€åŒ–ä¸ºäºšçº¿æ€§(1â†’logN)_

under a similar design. There are other challenges as well. 

**The latency issue** will **deteriorate(æ¶åŒ–)** with **thread pools and task queues**, 
as the book keeping effort delegates tasks between the worker threads.

And locks can force context-switches which make things even worse.  
å»¶è¿Ÿé—®é¢˜ä¼šéšç€çº¿ç¨‹æ± å’Œä»»åŠ¡é˜Ÿåˆ—çš„å‡ºç°è€Œæ¶åŒ–ï¼Œå› ä¸ºè®°è´¦å·¥ä½œä¼šåœ¨å·¥ä½œçº¿ç¨‹ä¹‹é—´å§”æ´¾ä»»åŠ¡ã€‚
é”å¯ä»¥å¼ºåˆ¶ä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œè¿™ä¼šä½¿æƒ…å†µå˜å¾—æ›´ç³Ÿã€‚

The Crimson project wishes to **address** the CPU **scalability** issue with the **shared-nothing design** and run-to-completion model. 

Crimson é¡¹ç›®å¸Œæœ›é€šè¿‡æ— å…±äº«è®¾è®¡å’Œè¿è¡Œåˆ°å®Œæˆæ¨¡å‹æ¥è§£å†³ CPU å¯æ‰©å±•æ€§é—®é¢˜

| è‹±æ–‡æœ¯è¯­                        | ä¸­æ–‡å«ä¹‰     | è¯´æ˜                                               |
| --------------------------- | -------- | ------------------------------------------------ |
| **address**                 | è§£å†³ï¼›åº”å¯¹    | åœ¨æ­¤æŒ‡"å¤„ç†å¹¶æ¶ˆé™¤"æŸä¸€æŠ€æœ¯éš¾é¢˜ã€‚                                |
| **CPU scalability**         | CPU å¯æ‰©å±•æ€§ | æŒ‡ç³»ç»Ÿåœ¨å¢åŠ æ›´å¤š CPU èµ„æºæ—¶ï¼Œæ€§èƒ½ï¼ˆååé‡ã€å¹¶å‘å¤„ç†èƒ½åŠ›ï¼‰èƒ½å¦æˆæ¯”ä¾‹æå‡ã€‚          |
| **shared-nothing design**   | å…±äº«æ— è®¾è®¡    | åˆ†å¸ƒå¼/å¹¶è¡Œç³»ç»Ÿè®¾è®¡åŸåˆ™ï¼Œç»„ä»¶é—´ä¸å…±äº«èµ„æºï¼Œé€šè¿‡æ¶ˆæ¯ä¼ é€’å®ç°åä½œã€‚                |
| **run-to-completion model** | è¿è¡Œåˆ°å®Œæˆæ¨¡å‹  | ä»»åŠ¡åœ¨åŒä¸€æ‰§è¡Œä¸Šä¸‹æ–‡ä¸­è¿ç»­æ‰§è¡Œç›´è‡³å®Œæˆï¼Œé¿å…ä¸­é€”åˆ‡æ¢ï¼Œä»¥é™ä½è°ƒåº¦å’Œä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€ã€‚        |
| **event-driven framework**  | äº‹ä»¶é©±åŠ¨æ¡†æ¶   | é€šè¿‡äº‹ä»¶å¾ªç¯å’Œå›è°ƒæœºåˆ¶ç®¡ç†ä»»åŠ¡ï¼Œå¯ä¸ run-to-completion æ¨¡å¼ç»“åˆï¼Œå®ç°é«˜å¹¶å‘ã€‚ |
| **user-space scheduling**   | ç”¨æˆ·æ€è°ƒåº¦    | å°†ä»»åŠ¡è°ƒåº¦é€»è¾‘æ”¾åœ¨ç”¨æˆ·ç©ºé—´ï¼Œå‡å°‘å¯¹å†…æ ¸è°ƒåº¦çš„ä¾èµ–ï¼Œæå‡æ•ˆç‡å’Œå¯æ§æ€§ã€‚               |


Basically, the design is to enforce each core, or CPU, to run one pinned thread and schedule **non-blocking tasks in user space**.
åŸºæœ¬ä¸Šï¼Œ
è¯¥è®¾è®¡æ˜¯å¼ºåˆ¶æ¯ä¸ªå†…æ ¸æˆ– CPU è¿è¡Œä¸€ä¸ªå›ºå®šçº¿ç¨‹ï¼Œå¹¶åœ¨ç”¨æˆ·ç©ºé—´ä¸­è°ƒåº¦éé˜»å¡ä»»åŠ¡


Request**s as well as** their resources are sharded by cores, 
è¯·æ±‚åŠå…¶èµ„æºæŒ‰å†…æ ¸åˆ†ç‰‡

so they can be processed in the same core until completion. 
å› æ­¤å®ƒä»¬å¯ä»¥åœ¨åŒä¸€å†…æ ¸ä¸­è¿›è¡Œå¤„ç†ï¼Œç›´åˆ°å®Œæˆ

Ideally, all the locks and context-switches are no longer needed as each running non-blocking task owns the CPU until it completes or cooperatively yields. 
ç†æƒ³æƒ…å†µä¸‹ï¼Œä¸å†éœ€è¦æ‰€æœ‰çš„é”å’Œä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œ
å› ä¸ºæ¯ä¸ªæ­£åœ¨è¿è¡Œçš„éé˜»å¡ä»»åŠ¡éƒ½æ‹¥æœ‰ CPUï¼Œç›´åˆ°å®ƒå®Œæˆæˆ–åä½œäº§ç”Ÿã€‚æ²¡æœ‰å…¶ä»–çº¿ç¨‹å¯ä»¥åŒæ—¶æŠ¢å ä»»åŠ¡

![image.png](https://s2.loli.net/2025/05/30/Kv5ahBcU2LzTXqF.png)


![image.png](https://s2.loli.net/2025/05/30/og42IetcNaj7mPL.png)



Although **cross-shard communications** cannot be eliminated entirely, those are usually for OSD global status maintenance and not in the data path.
è™½ç„¶æ— æ³•å®Œå…¨æ¶ˆé™¤è·¨åˆ†ç‰‡é€šä¿¡ï¼Œä½†è¿™äº›é€šä¿¡é€šå¸¸ç”¨äº OSD å…¨å±€çŠ¶æ€ç»´æŠ¤ï¼Œè€Œä¸æ˜¯åœ¨æ•°æ®è·¯å¾„ä¸­

A major challenge here is that the expected changes are fundamental to core OSD operation â€“ a considerable part of the existing locking or threading code cannot be reused, and needs to be redesigned while backward compatibility is maintained. 

è¿™é‡Œçš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯é¢„æœŸçš„æ›´æ”¹æ˜¯æ ¸å¿ƒ OSD ä½œçš„åŸºç¡€ â€“ ç°æœ‰é”å®šæˆ–çº¿ç¨‹ä»£ç çš„å¾ˆå¤§ä¸€éƒ¨åˆ†æ— æ³•é‡ç”¨ï¼Œéœ€è¦åœ¨ä¿æŒå‘åå…¼å®¹æ€§çš„åŒæ—¶é‡æ–°è®¾è®¡


Redesign requires holistic understanding of the code with inherent caveats.

It is another challenge to implement low-level thread-per-core and user-space scheduling using shared-nothing architecture. 

é‡æ–°è®¾è®¡éœ€è¦å¯¹ä»£ç æœ‰æ•´ä½“çš„ç†è§£ï¼Œä½†æœ‰å›ºæœ‰çš„æ³¨æ„äº‹é¡¹ã€‚ä½¿ç”¨æ— å…±äº«æ¶æ„å®ç°ä½çº§æ¯æ ¸çº¿ç¨‹å’Œç”¨æˆ·ç©ºé—´è°ƒåº¦æ˜¯å¦ä¸€ä¸ªæŒ‘æˆ˜


Crimson seeks to build the **redesigned** OSD based on Seastar, an asynchronous programming framework with all the ideal characteristics to meet the above goals.  

>Crimson å¯»æ±‚åŸºäº Seastar æ„å»ºé‡æ–°è®¾è®¡çš„ OSDï¼ŒSeastar æ˜¯ä¸€ä¸ªå¼‚æ­¥ç¼–ç¨‹æ¡†æ¶ï¼Œ
å…·æœ‰æ»¡è¶³ä¸Šè¿°ç›®æ ‡çš„æ‰€æœ‰ç†æƒ³ç‰¹æ€§ã€‚

**Crimson OSD ç¤ºä¾‹ä»£ç **ï¼Œå±•ç¤ºå¦‚ä½•åŸºäº Seastar æ„å»ºä¸€ä¸ªç®€å•çš„å¼‚æ­¥æœåŠ¡

# Seastar FrameworkÂ Â Seastar æ¡†æ¶

SeastarÂ **is ideal for** the Crimson project because it not only implements the **one-thread-per-core** shared-nothing infrastructure in C++, 

SeastarÂ æ˜¯ Crimson é¡¹ç›®çš„ç†æƒ³é€‰æ‹©ï¼Œå› ä¸ºå®ƒä¸ä»…åœ¨ C++ ä¸­å®ç°äº†æ¯æ ¸ä¸€çº¿ç¨‹çš„æ— å…±äº«åŸºç¡€è®¾æ–½ï¼Œ


but also provides a comprehensive set of features and models that have been proven to be effective for performance and scaling in other applications.
è€Œä¸”è¿˜æä¾›äº†ä¸€å¥—å…¨é¢çš„åŠŸèƒ½å’Œæ¨¡å‹ï¼Œè¿™äº›åŠŸèƒ½å’Œæ¨¡å‹å·²è¢«è¯æ˜å¯¹å…¶ä»–åº”ç”¨ç¨‹åºä¸­çš„æ€§èƒ½å’Œæ‰©å±•æœ‰æ•ˆã€‚

Resources are by-default not shared between shards, and Seastar implements its own **memory allocato**r for lockless allocations. 
é»˜è®¤æƒ…å†µä¸‹ï¼Œèµ„æºä¸åœ¨åˆ†ç‰‡ä¹‹é—´å…±äº«ï¼ŒSeastar å®ç°äº†è‡ªå·±çš„å†…å­˜åˆ†é…å™¨ï¼Œç”¨äºæ— é”åˆ†é…


The allocator also **takes advantage of** the **NUMA topology** to assign the closest memory to the shard. 
åˆ†é…å™¨è¿˜åˆ©ç”¨ NUMA æ‹“æ‰‘å°†æœ€è¿‘çš„å†…å­˜åˆ†é…ç»™åˆ†ç‰‡


For some inevitable cross-core resource sharing and communications, Seastar enforces them to be handled explicitly: If a shard owns resources from another core, it must hold them by foreign pointers; And if a shard needs to communicate with other shards, it must submit and forward the tasks to them. 
å¯¹äºä¸€äº›ä¸å¯é¿å…çš„è·¨æ ¸èµ„æºå…±äº«å’Œé€šä¿¡ï¼ŒSeastar å¼ºåˆ¶æ˜¾å¼å¤„ç†å®ƒä»¬ï¼šå¦‚æœä¸€ä¸ªåˆ†ç‰‡æ‹¥æœ‰æ¥è‡ªå¦ä¸€ä¸ªæ ¸å¿ƒçš„èµ„æºï¼Œå®ƒå¿…é¡»é€šè¿‡å¤–éƒ¨æŒ‡é’ˆæ¥ä¿å­˜å®ƒä»¬
;å¦‚æœä¸€ä¸ªåˆ†ç‰‡éœ€è¦ä¸å…¶ä»–åˆ†ç‰‡é€šä¿¡ï¼Œåˆ™å¿…é¡»å‘å®ƒä»¬æäº¤å’Œè½¬å‘ä»»åŠ¡ã€‚

This forces the programmers to restrain their needs to go cross-core and helps to reduce the analysis scope for the CPU scalability issues. 
è¿™è¿«ä½¿ç¨‹åºå‘˜å…‹åˆ¶ä»–ä»¬è·¨æ ¸çš„éœ€æ±‚ï¼Œå¹¶æœ‰åŠ©äºç¼©å° CPU å¯æ‰©å±•æ€§é—®é¢˜çš„åˆ†æèŒƒå›´ã€‚

Seastar also implements high-performance **non-blocking primitives** for the inevitable cross-core communications.
Seastar è¿˜ä¸ºä¸å¯é¿å…çš„è·¨æ ¸é€šä¿¡å®ç°äº†é«˜æ€§èƒ½çš„éé˜»å¡åŸè¯­ã€‚





![image.png](https://s2.loli.net/2025/05/30/97l3VN2e4qcrKx5.png)

The conventional program with asynchronous events and callbacks is notoriously challenging to implement, understand and debug.
ä¼—æ‰€å‘¨çŸ¥ï¼Œå…·æœ‰å¼‚æ­¥äº‹ä»¶å’Œå›è°ƒçš„ä¼ ç»Ÿç¨‹åºåœ¨å®ç°ã€ç†è§£å’Œè°ƒè¯•æ–¹é¢éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§

And non-blocking task scheduling in user space requires implementation to be pervasively asynchronous. 
ç”¨æˆ·ç©ºé—´ä¸­çš„éé˜»å¡ä»»åŠ¡è°ƒåº¦éœ€è¦æ™®éå¼‚æ­¥çš„å®ç°ã€‚


Seastar implements the futures, promises and continuations (f/p/c) as the building blocks to organize the logic. 
Seastar å®ç°äº† futuresã€promise å’Œ continuations ï¼ˆf/p/cï¼‰ ä½œä¸ºç»„ç»‡é€»è¾‘çš„æ„å»ºå—ã€‚

ç±»åˆ«ï¼šc++11 ç‰¹æ€§å‘€ï¼Œå’Œå…¶ä»–è¯­è¨€ä¹Ÿè¿™æ ·ç‰¹æ€§

The futures and promises makes code easier to write and read by grouping logically connected, asynchronous constructs together, rather than scattering them in plain callbacks. 

futures å’Œ promise é€šè¿‡å°†é€»è¾‘è¿æ¥çš„å¼‚æ­¥ç»“æ„åˆ†ç»„åœ¨ä¸€èµ·ï¼Œè€Œä¸æ˜¯å°†å®ƒä»¬åˆ†æ•£åœ¨æ™®é€šå›è°ƒä¸­ï¼Œä½¿ä»£ç æ›´æ˜“äºç¼–å†™å’Œé˜…è¯»ã€‚


Seastar also provides higher level facilities for loops, timers, and to control lifecycles and even CPU shares based on the futures. 
Seastar è¿˜ä¸ºå¾ªç¯ã€è®¡æ—¶å™¨ä»¥åŠåŸºäºæœŸè´§çš„ç”Ÿå‘½å‘¨æœŸç”šè‡³ CPU ä»½é¢æä¾›äº†æ›´é«˜çº§åˆ«çš„å·¥å…·


To further simplify applications, Seastar encapsulates both networking and disk accesses to the world of shared-nothing and f/p/c-based design. 
ä¸ºäº†è¿›ä¸€æ­¥ç®€åŒ–åº”ç”¨ç¨‹åºï¼ŒSeastar å°†ç½‘ç»œå’Œç£ç›˜è®¿é—®å°è£…åˆ°æ— å…±äº«å’ŒåŸºäº f/p/c çš„è®¾è®¡ä¸–ç•Œä¸­

ç¬”è®°ï¼šè¿™ä¸ª st åç¨‹ä¸€æ ·ï¼Œå¦å¤–ä¸€ä¸ªè½®å­ ä¸è¦è¿˜æ²¡è¯´åŸºç¡€è¿‡ï¼Œæœ¬è´¨ä¸€æ ·ï¼Œ

The complexity and nuanced control to adopt different I/O stacks (such as epoll, linux-aio, io-uring, DPDK, etc) **are transparent** to the application code.  

é‡‡ç”¨ä¸åŒ I/O å †æ ˆï¼ˆå¦‚ epollã€linux-aioã€io-uringã€DPDK ç­‰ï¼‰çš„å¤æ‚æ€§å’Œç»†å¾®æ§åˆ¶å¯¹åº”ç”¨ç¨‹åºä»£ç æ˜¯é€æ˜çš„ã€‚

ç–‘é—®ï¼šä»€ä¹ˆæ„æ€ ï¼Œç”¨æˆ·ä¸ç”¨ç®¡åªè°ƒç”¨ç›¸å…³æ¥å£
- Seastar åœ¨å†…éƒ¨ä¼šæ ¹æ®å†…æ ¸ç‰ˆæœ¬å’Œé…ç½®ï¼Œè‡ªåŠ¨é€‰æ‹©ä½¿ç”¨ `linux-aio`ã€`io-uring` æˆ–è€…å›é€€åˆ°åŸºäº `epoll` çš„å¼‚æ­¥å®ç°ã€‚
- åº”ç”¨æ— éœ€ä¿®æ”¹ä»»ä½•ä¸€è¡Œï¼Œå°±å¯ä»¥åœ¨ä¸åŒç³»ç»Ÿä¸Šå–å¾—æœ€ä¼˜ I/O æ€§èƒ½ã€‚
- **ä¸Šå±‚ä»£ç åªè·Ÿ `listen`ã€`socket::send`ã€`packet` ç­‰ Seastar ç½‘ç»œ API æ‰“äº¤é“**ã€‚
- Seastar å†…éƒ¨å¯åŸºäº `DPDK`ï¼ˆåœ¨é…ç½®å¯ç”¨æ—¶ï¼‰æˆ–å†…æ ¸ `epoll`/`io-uring` å®ç°é«˜æ€§èƒ½ç”¨æˆ·æ€/å†…æ ¸æ€ I/Oã€‚
- å¯¹åº”ç”¨æ¥è¯´ï¼Œå®Œå…¨"é€æ˜"ï¼šæ— éœ€å…³å¿ƒåº•å±‚æ˜¯ç”¨å“ªç§ç½‘ç»œæ¡†æ¶
![i](https://s2.loli.net/2025/05/30/SZcItjLWVpFKlD7.png)

## Run-to-completion performance  è¿è¡Œåˆ°å®Œæˆæ€§èƒ½


The Crimson team has implemented most of the critical features of OSD for **read and write workloads** from RBD clients. 
Crimson å›¢é˜Ÿå·²ç»ä¸º RBD å®¢æˆ·ç«¯çš„è¯»å–å’Œå†™å…¥å·¥ä½œè´Ÿè½½å®æ–½äº† OSD çš„å¤§éƒ¨åˆ†å…³é”®åŠŸèƒ½

The effort includes re-implementations of the messenger V2 (msgr2) protocol, heartbeat, PG peering, backfill, recovery, object-classes, watch-notify, etc., with continuous stabilization efforts to add CI test suites. 
è¿™é¡¹å·¥ä½œåŒ…æ‹¬é‡æ–°å®ç°ä¿¡ä½¿ V2 ï¼ˆmsgr2ï¼‰ åè®®ã€å¿ƒè·³ã€PG å¯¹ç­‰ã€å›å¡«ã€æ¢å¤ã€å¯¹è±¡ç±»ã€ç›‘è§†é€šçŸ¥ç­‰ï¼Œå¹¶æŒç»­ç¨³å®šå·¥ä½œä»¥æ·»åŠ  CI æµ‹è¯•å¥—ä»¶ã€‚
 
 Crimson has reached a milestone where we can validate run-to-completion design in a single shard with sufficient stability. 
 
Crimson å·²ç»è¾¾åˆ°äº†ä¸€ä¸ªé‡Œç¨‹ç¢‘ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å•ä¸ªåˆ†ç‰‡ä¸­éªŒè¯å…·æœ‰è¶³å¤Ÿç¨³å®šæ€§çš„è¿è¡Œåˆ°å®Œæˆè®¾è®¡ã€‚

Considering **fairness** and **reality**, the single-shard run-to-completion is validated by comparing Classic and Crimson OSD with the **BlueStore backend**,
under the same random 4KB RBD workload, and without replication. 
è€ƒè™‘åˆ°å…¬å¹³æ€§å’Œç°å®æ€§ï¼Œåœ¨ç›¸åŒçš„éšæœº 4KB RBD å·¥ä½œè´Ÿè½½ä¸‹ï¼Œé€šè¿‡å°† Classic å’Œ Crimson OSD ä¸ 

BlueStore åç«¯è¿›è¡Œæ¯”è¾ƒæ¥éªŒè¯å•åˆ†ç‰‡è¿è¡Œåˆ°å®Œæˆï¼Œå¹¶ä¸”æ— éœ€å¤åˆ¶



Both OSDs are assigned 2 CPU resources. 
ä¸¤ä¸ª OSD éƒ½åˆ†é…äº† 2 ä¸ª CPU èµ„æº


The Crimson OSD is special because Seastar requires an exclusive core to run the single-shard OSD logic in the reactor. 

It implies that the BlueStore threads must be pinned to the other core, with AlienStore introduced to bridge the boundary between the Seastar thread and BlueStore threads and to submit IO tasks between the two worlds. 


In contrast, the Classic OSD has no restrictions to use the assigned 2 CPUs.  
ã€‚Crimson OSD å¾ˆç‰¹åˆ«ï¼Œå› ä¸º Seastar éœ€è¦ä¸€ä¸ªä¸“ç”¨å†…æ ¸æ‰èƒ½åœ¨ååº”å™¨ä¸­è¿è¡Œå•åˆ†ç‰‡ OSD é€»è¾‘ã€‚è¿™æ„å‘³ç€ BlueStore çº¿ç¨‹å¿…é¡»å›ºå®šåˆ°å¦ä¸€ä¸ªå†…æ ¸ï¼Œå¼•å…¥ AlienStore æ¥å¼¥åˆ Seastar çº¿ç¨‹å’Œ 

BlueStore çº¿ç¨‹ä¹‹é—´çš„è¾¹ç•Œï¼Œå¹¶åœ¨ä¸¤ä¸ªä¸–ç•Œä¹‹é—´æäº¤ IO ä»»åŠ¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒClassic OSD å¯¹ä½¿ç”¨åˆ†é…çš„ 2 ä¸ª CPU æ²¡æœ‰é™åˆ¶ã€‚

![](https://s2.loli.net/2025/05/30/mOojdg2nrfpq5Xs.png)

The performance result shows that with BlueStore, the Crimson OSD has roughly **25%** better performance for random-reads and has about 24% better IOPS than the Classic OSD for the random-write case.

Further analysis shows that the CPU is underutilized in the random-write scenario, as ~20% CPU is consumed in busy-polling, suggesting Crimson OSD is not the likely bottleneck here.  
æ€§èƒ½ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ BlueStore æ—¶ï¼ŒCrimson OSD çš„éšæœºè¯»å–æ€§èƒ½æ¯” Classic OSD é«˜çº¦ 25%ï¼Œä¸ç»å…¸ OSD ç›¸æ¯”ï¼Œéšæœºå†™å…¥æƒ…å†µçš„ IOPS é«˜çº¦ 24%ã€‚

è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œåœ¨éšæœºå†™å…¥åœºæ™¯ä¸­ï¼ŒCPU æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œå› ä¸º ~20% çš„ CPU åœ¨å¿™è½®è¯¢ä¸­æ¶ˆè€—ï¼Œè¿™è¡¨æ˜ Crimson OSD ä¸æ˜¯è¿™é‡Œå¯èƒ½çš„ç“¶é¢ˆã€‚

![bluestore-performance](https://s2.loli.net/2025/05/30/5foONUVSLyZR4WH.png)


There is also extra **overhead** for the Crimson OSD to submit and complete IO tasks, and synchronize between the Seastar and BlueStore threads. 


So, we repeat the same set of experiments against the MemStore backend, with both OSDs assigned 1 CPU. Crimson OSD delivers about 70% better IOPS in random-read and is about 25% better than the Classic OSD in the random-write case, as shown below, agreeing with the assertion in the previous experiment that Crimson OSD could do better.  


Crimson OSD æäº¤å’Œå®Œæˆ IO ä»»åŠ¡ä»¥åŠåœ¨ Seastar å’Œ BlueStore çº¿ç¨‹ä¹‹é—´åŒæ­¥ä¹Ÿä¼šäº§ç”Ÿé¢å¤–çš„å¼€é”€ã€‚
å› æ­¤ï¼Œæˆ‘ä»¬å¯¹ MemStore åç«¯é‡å¤åŒä¸€ç»„å®éªŒï¼Œä¸¤ä¸ª OSD éƒ½åˆ†é…äº† 1 ä¸ª CPUã€‚Crimson OSD åœ¨éšæœºè¯»å–ä¸­æä¾›çš„ IOPS æé«˜äº†çº¦ 70%ï¼Œåœ¨éšæœºå†™å…¥æƒ…å†µä¸‹æ¯”ç»å…¸ OSD é«˜å‡ºçº¦ 25%ï¼Œ
å¦‚ä¸‹æ‰€ç¤ºï¼Œè¿™ä¸ä¹‹å‰å®éªŒä¸­å…³äº Crimson OSD å¯ä»¥åšå¾—æ›´å¥½çš„æ–­è¨€ä¸€è‡´ã€‚

# Multi-shard Implementation  å¤šåˆ†ç‰‡å®ç°

The path towards a multiple-shard implementation is clear. 
å®ç°å¤šåˆ†ç‰‡å®ç°çš„é“è·¯å¾ˆæ˜ç¡®ã€‚

Since IO in each PG is already logically sharded, there is no significant change to the IO path. 
ç”±äºæ¯ä¸ª PG ä¸­çš„ IO å·²åœ¨é€»è¾‘ä¸Šåˆ†ç‰‡ï¼Œå› æ­¤ IO è·¯å¾„æ²¡æœ‰é‡å¤§å˜åŒ–

The major challenge is to identify places where cross-core communication is inevitable, and design new solutions to minimize the exposure and its impact to the IO path, which will need to be on a case-by-case basis. 
ä¸»è¦æŒ‘æˆ˜æ˜¯ç¡®å®šè·¨æ ¸é€šä¿¡ä¸å¯é¿å…çš„ä½ç½®ï¼Œå¹¶è®¾è®¡æ–°çš„è§£å†³æ–¹æ¡ˆä»¥æœ€å¤§é™åº¦åœ°å‡å°‘æš´éœ²åŠå…¶å¯¹ IO è·¯å¾„çš„å½±å“ï¼Œè¿™éœ€è¦æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œ

In general, when an IO operation is received from the Messenger, it will be directed to the OSD shard according to the PG-core mapping, and will run in the context of the same shard/CPU until completion. 
é€šå¸¸ï¼Œå½“ä» Messenger æ”¶åˆ° IO ä½œæ—¶ï¼Œå°†æ ¹æ® PG æ ¸å¿ƒæ˜ å°„å°†å…¶å®šå‘åˆ° OSD åˆ†ç‰‡ï¼Œå¹¶å°†åœ¨åŒä¸€åˆ†ç‰‡/CPU çš„ä¸Šä¸‹æ–‡ä¸­è¿è¡Œï¼Œç›´åˆ°å®Œæˆ


Note that in the current phase the design choice not to modify the RADOS protocol has been made for simplicity.  
è¯·æ³¨æ„ï¼Œåœ¨å½“å‰é˜¶æ®µï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œå·²é€‰æ‹©ä¸ä¿®æ”¹ RADOS åè®®ã€‚

![](https://s2.loli.net/2025/05/30/ALYqr1QakxBwMhK.png)

### ç”¨ä¸€ä¸ªæ¯”å–»è¯´æ˜

- **å•æ ˆæ»‘é›ªé“**ï¼šæ‰€æœ‰æ»‘é›ªè€…ï¼ˆè¯·æ±‚ï¼‰éƒ½åœ¨åŒä¸€æ¡æ»‘é“ï¼ˆçº¿ç¨‹/æ ¸ï¼‰ä¸Šæ»‘ä¸‹æ¥ï¼Œæ’é˜Ÿç­‰å¾…ã€‚
    
- **å¤šæ ˆæ»‘é›ªé“**ï¼šæ ¹æ®æ»‘é›ªè€…çš„ç¼–å·ï¼ˆPG IDï¼‰ï¼Œä»–ä»¬è¢«åˆ†é…åˆ°ä¸åŒçš„æ»‘é“ï¼ˆæ ¸å¿ƒï¼‰ä¸Šå»æ»‘ã€‚
    
    - **æ»‘é“å†…éƒ¨**ï¼šæ‹å¼¯ã€åŠ é€Ÿç­‰æ“ä½œï¼ˆIO é€»è¾‘ï¼‰éƒ½ä¸€æ ·ï¼Œä¸ç®¡ä½ åœ¨å“ªæ¡æ»‘é“ã€‚
        
    - **æ»‘é“ä¹‹é—´**ï¼šæå°‘æ•°æ»‘é›ªè€…ä¼šè¢«ç§»åˆ°å¦ä¸€æ¡æ»‘é“ï¼ˆè·¨æ ¸é€šä¿¡ï¼Œæ¯”å¦‚å…¨å±€çŠ¶æ€æ›´æ–°ï¼‰ï¼Œä½†è¿™ä¸å¹²æ‰°"æ»‘é›ªä½“éªŒä¸»æµç¨‹"ã€‚





## OSDÂ 

OSD is responsible for maintaining global status and activities shared between PG shards, including heartbeat, authentication, client management, osdmap, PG maintenance, access to the Messenger and ObjectStore, and so on.  
OSD è´Ÿè´£ç»´æŠ¤ PG åˆ†ç‰‡ä¹‹é—´å…±äº«çš„å…¨å±€çŠ¶æ€å’Œæ´»åŠ¨ï¼ŒåŒ…æ‹¬æ£€æµ‹ä¿¡å·ã€èº«ä»½éªŒè¯ã€å®¢æˆ·ç«¯ç®¡ç†ã€osdmapã€PG ç»´æŠ¤ã€å¯¹ Messenger å’Œ ObjectStore çš„è®¿é—®ç­‰ã€‚

A simple principle towards a multi-core Crimson OSD is to keep all processing relating to shared state on a dedicated core. If an IO operation wants to access the shared resource, it either needs to access the dedicated core in order, or to access an exclusive copy of the shared information that is kept synchronized.  
å¤šæ ¸ Crimson OSD çš„ä¸€ä¸ªç®€å•åŸåˆ™æ˜¯å°†ä¸å…±äº«çŠ¶æ€ç›¸å…³çš„æ‰€æœ‰å¤„ç†éƒ½ä¿å­˜åœ¨ä¸“ç”¨å†…æ ¸ä¸Šã€‚å¦‚æœ IO ä½œæƒ³è¦è®¿é—®å…±äº«èµ„æºï¼Œå®ƒè¦ä¹ˆéœ€è¦æŒ‰é¡ºåºè®¿é—®ä¸“ç”¨æ ¸å¿ƒï¼Œè¦ä¹ˆè®¿é—®ä¿æŒåŒæ­¥çš„å…±äº«ä¿¡æ¯çš„ç‹¬å å‰¯æœ¬ã€‚

There are two major steps to achieve this goal. The first step is to allow IO operations to run in multiple OSD shards according to the PG sharding policy, with all the global information including PG status maintained in the first shard. This step enables sharding in the OSD but requires all the decisions about IO dispatching to be made in the first shard. Even if the Messenger can run in multiple cores at this step, messages will still need to be delivered to the first shard for preparation (PG peering, as an example) and determining the correct PG shard before being submitted to that shard. This can result in extra overhead and unbalanced CPU utilization (the first OSD shard is busy and other shards are starved, etc). So, the next step is to extend the PG-core mapping to all the OSD shards, so that the received message can be dispatched to the correct shard directly.  

å®ç°æ­¤ç›®æ ‡æœ‰ä¸¤ä¸ªä¸»è¦æ­¥éª¤ã€‚ç¬¬ä¸€æ­¥æ˜¯å…è®¸ IO ä½œæ ¹æ® PG åˆ†ç‰‡ç­–ç•¥åœ¨å¤šä¸ª OSD åˆ†ç‰‡ä¸­è¿è¡Œï¼ŒåŒ…æ‹¬ PG çŠ¶æ€åœ¨å†…çš„æ‰€æœ‰å…¨å±€ä¿¡æ¯éƒ½ä¿ç•™åœ¨ç¬¬ä¸€ä¸ªåˆ†ç‰‡ä¸­ã€‚æ­¤æ­¥éª¤åœ¨ OSD ä¸­å¯ç”¨åˆ†ç‰‡ï¼Œä½†è¦æ±‚åœ¨ç¬¬ä¸€ä¸ªåˆ†ç‰‡ä¸­åšå‡ºæœ‰å…³ IO è°ƒåº¦çš„æ‰€æœ‰å†³ç­–ã€‚å³ä½¿ Messenger å¯ä»¥åœ¨æ­¤æ­¥éª¤ä¸­åœ¨å¤šä¸ªå†…æ ¸ä¸­è¿è¡Œï¼Œæ¶ˆæ¯ä»éœ€è¦ä¼ é€åˆ°ç¬¬ä¸€ä¸ªåˆ†ç‰‡è¿›è¡Œå‡†å¤‡ï¼ˆä¾‹å¦‚ PG å¯¹ç­‰è¿æ¥ï¼‰å¹¶ç¡®å®šæ­£ç¡®çš„ PG åˆ†ç‰‡ï¼Œç„¶åå†æäº¤åˆ°è¯¥åˆ†ç‰‡ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´é¢å¤–çš„å¼€é”€å’Œ CPU åˆ©ç”¨ç‡ä¸å¹³è¡¡ï¼ˆç¬¬ä¸€ä¸ª OSD åˆ†ç‰‡ç¹å¿™ï¼Œå…¶ä»–åˆ†ç‰‡ä¸è¶³ç­‰ï¼‰ã€‚å› æ­¤ï¼Œä¸‹ä¸€æ­¥æ˜¯å°† PG-core æ˜ å°„æ‰©å±•åˆ°æ‰€æœ‰ OSD åˆ†ç‰‡ï¼Œä»¥ä¾¿å¯ä»¥å°†æ”¶åˆ°çš„æ¶ˆæ¯ç›´æ¥åˆ†æ´¾åˆ°æ­£ç¡®çš„åˆ†ç‰‡ã€‚

## ObjectStoreÂ [Â¶](https://wiki.ceph.com/en/news/blog/2023/crimson-multi-core-scalability/?utm_source=chatgpt.com#objectstore)Â Â ObjectStoreÂ [ï¼ˆ](https://wiki.ceph.com/en/news/blog/2023/crimson-multi-core-scalability/?utm_source=chatgpt.com#objectstore)Â å¯¹è±¡å­˜å‚¨ï¼‰ Â¶

There are three ObjectStore backends supported for Crimson: 
AlienStore, 
CyanStore and 
SeaStore

AlienStore provides backward compatibility with BlueStore
. CyanStore is a dummy backend for tests, implemented by volatile memory. SeaStore is a new object store designed specifically for Crimson OSD with shared-nothing design. The paths towards multiple shard support are different depending on the specific goal of the backend.  

Crimson æ”¯æŒä¸‰ä¸ª ObjectStore åç«¯ï¼šAlienStoreã€CyanStore å’Œ SeaStoreã€‚AlienStore æä¾›ä¸ BlueStore çš„å‘åå…¼å®¹æ€§ã€‚CyanStore æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•çš„è™šæ‹Ÿåç«¯ï¼Œ
ç”± volatile å†…å­˜å®ç°ã€‚SeaStore æ˜¯ä¸“ä¸º Crimson OSD è®¾è®¡çš„æ–°å¯¹è±¡å­˜å‚¨ï¼Œé‡‡ç”¨æ— å…±äº«è®¾è®¡ã€‚æ ¹æ®åç«¯çš„å…·ä½“ç›®æ ‡ï¼Œå®ç°å¤šä¸ªåˆ†ç‰‡æ”¯æŒçš„è·¯å¾„ä¼šæœ‰æ‰€ä¸åŒã€‚

### AlienStoreÂ 




AlienStore is a thin proxy in the Seastar thread to communicate with BlueStore that uses POSIX threads, (alien world from a Seastar perspective). There is no special work to do towards multiple OSD shards because the IO tasks are already synchronized for alien world communications. There are no Crimson specific customizations in the BlueStore because it is impossible to truly extend the BlueStore into shared-nothing design as it depends on the 3rd party RocksDB project which is still threaded. However, it is acceptable to pay reasonable overhead in exchange for a sophisticated storage backend solution, before Crimson can come up with a native storage backend solution (SeaStore) which is optimized and stable enough.  
AlienStore æ˜¯ Seastar çº¿ç¨‹ä¸­çš„ä¸€ä¸ªç˜¦ä»£ç†ï¼Œç”¨äºä¸ä½¿ç”¨ POSIX çº¿ç¨‹çš„ BlueStore é€šä¿¡ï¼ˆä» Seastar çš„è§’åº¦æ¥çœ‹æ˜¯å¤–æ˜Ÿä¸–ç•Œï¼‰ã€‚å¯¹äºå¤šä¸ª OSD åˆ†ç‰‡ï¼Œæ— éœ€æ‰§è¡Œç‰¹æ®Šå·¥ä½œï¼Œå› ä¸º IO ä»»åŠ¡å·²ä¸å¤–æ˜Ÿä¸–ç•Œé€šä¿¡åŒæ­¥ã€‚BlueStore ä¸­æ²¡æœ‰ç‰¹å®šäº Crimson çš„è‡ªå®šä¹‰ï¼Œå› ä¸ºä¸å¯èƒ½çœŸæ­£å°† BlueStore æ‰©å±•åˆ°æ— å…±äº«è®¾è®¡ä¸­ï¼Œå› ä¸ºå®ƒä¾èµ–äºä»ç„¶çº¿ç¨‹åŒ–çš„ç¬¬ä¸‰æ–¹ RocksDB é¡¹ç›®ã€‚ä½†æ˜¯ï¼Œåœ¨ Crimson æå‡ºè¶³å¤Ÿä¼˜åŒ–å’Œç¨³å®šçš„æœ¬æœºå­˜å‚¨åç«¯è§£å†³æ–¹æ¡ˆ ï¼ˆSeaStoreï¼‰ ä¹‹å‰ï¼Œæ”¯ä»˜åˆç†çš„å¼€é”€ä»¥æ¢å–å¤æ‚çš„å­˜å‚¨åç«¯è§£å†³æ–¹æ¡ˆæ˜¯å¯ä»¥æ¥å—çš„ã€‚

### CyanStoreÂ 

CyanStore in Crimson OSD is the counterpart of MemStore in the Classic OSD.

The only change towards multiple-shard support is to create independent CyanStore instances per shard. One goal is to make sure the dummy IO operation can complete in the same core to help identify OSD-level scalability issues if there are any. The other goal is to do direct performance comparisons with the Classic at the OSD level without complex impacts from the ObjectStore.  
Crimson OSD ä¸­çš„ CyanStore æ˜¯ç»å…¸ OSD ä¸­ MemStore çš„å¯¹åº”ç‰©]ã€‚

å¤šåˆ†ç‰‡æ”¯æŒçš„å”¯ä¸€å˜åŒ–æ˜¯ä¸ºæ¯ä¸ªåˆ†ç‰‡åˆ›å»ºç‹¬ç«‹çš„ CyanStore å®ä¾‹ã€‚ä¸€ä¸ªç›®æ ‡æ˜¯ç¡®ä¿è™šæ‹Ÿ IO ä½œå¯ä»¥åœ¨åŒä¸€å†…æ ¸ä¸­å®Œæˆï¼Œä»¥å¸®åŠ©è¯†åˆ« OSD çº§åˆ«çš„å¯æ‰©å±•æ€§é—®é¢˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚å¦ä¸€ä¸ªç›®æ ‡æ˜¯åœ¨ OSD çº§åˆ«ä¸ Classic è¿›è¡Œç›´æ¥æ€§èƒ½æ¯”è¾ƒï¼Œè€Œä¸ä¼šå—åˆ° ObjectStore çš„å¤æ‚å½±å“ã€‚

### SeaStoreÂ 


SeaStore is the native ObjectStore solution for Crimson OSD, which is developed with the Seastar framework and adopts the same design principles.  
SeaStore æ˜¯ Crimson OSD çš„åŸç”Ÿ ObjectStore è§£å†³æ–¹æ¡ˆï¼Œå®ƒæ˜¯ä½¿ç”¨ Seastar æ¡†æ¶å¼€å‘çš„ï¼Œå¹¶é‡‡ç”¨ç›¸åŒçš„è®¾è®¡åŸåˆ™ã€‚

Although challenging, there are multiple reasons why Crimson must build a new local storage engine. Storage backend is the major CPU resource consumer, and the Crimson OSD cannot truly scale with cores if its storage backend cannot. Our experiment also proves that the Crimson OSD is not the bottleneck in the random write scenario.  
å°½ç®¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½† Crimson å¿…é¡»æ„å»ºæ–°çš„æœ¬åœ°å­˜å‚¨å¼•æ“çš„åŸå› æœ‰å¾ˆå¤šã€‚å­˜å‚¨åç«¯æ˜¯ä¸»è¦çš„ CPU èµ„æºæ¶ˆè€—è€…ï¼Œå¦‚æœ Crimson OSD çš„å­˜å‚¨åç«¯ä¸èƒ½ï¼Œåˆ™ Crimson OSD æ— æ³•çœŸæ­£ä½¿ç”¨å†…æ ¸è¿›è¡Œæ‰©å±•ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜è¯æ˜ï¼ŒCrimson OSD å¹¶ä¸æ˜¯éšæœºå†™å…¥åœºæ™¯ä¸­çš„ç“¶é¢ˆã€‚

Second, the CPU-intensive metadata managements with transaction support in the BlueStore are essentially provided by RocksDB, which cannot run in the native Seastar threads without reimplementation. 

Rather than re-implementing the common purpose key-value transactional storage for BlueStore, it is more reasonable to rethink and customize the according designs at a higher level - the ObjectStore. Issues can be solved more easily in the native solution than in the 3rd-party project which must be general to all the use cases that may be even unrelated.  

å…¶æ¬¡ï¼ŒBlueStore ä¸­å…·æœ‰äº‹åŠ¡æ”¯æŒçš„ CPU å¯†é›†å‹å…ƒæ•°æ®ç®¡ç†åŸºæœ¬ä¸Šæ˜¯ç”± RocksDB æä¾›çš„ï¼Œå¦‚æœä¸é‡æ–°å®ç°ï¼Œå®ƒå°±æ— æ³•åœ¨æœ¬åœ° Seastar çº¿ç¨‹ä¸­è¿è¡Œã€‚
ä¸å…¶ä¸º BlueStore é‡æ–°å®ç°é€šç”¨çš„é”®å€¼äº‹åŠ¡å­˜å‚¨ï¼Œä¸å¦‚åœ¨æ›´é«˜çº§åˆ« - ObjectStore é‡æ–°è€ƒè™‘å’Œè‡ªå®šä¹‰ç›¸åº”çš„è®¾è®¡ã€‚ä¸ç¬¬ä¸‰æ–¹é¡¹ç›®ç›¸æ¯”ï¼Œ
åœ¨æœ¬åœ°è§£å†³æ–¹æ¡ˆä¸­å¯ä»¥æ›´è½»æ¾åœ°è§£å†³é—®é¢˜ï¼Œå› ä¸ºç¬¬ä¸‰æ–¹é¡¹ç›®å¿…é¡»é€‚ç”¨äºæ‰€æœ‰ç”šè‡³å¯èƒ½ä¸ç›¸å…³çš„ç”¨ä¾‹ã€‚

The third consideration is to bring up native support to heterogeneous storage devices as well as hardware accelerators that will allow users to balance costs and performance according to their requirements. It will be more flexible for Crimson to simplify the solution to deploy the combination of hardware if it has better controls over the entire storage stack.  
ç¬¬ä¸‰ä¸ªè€ƒè™‘å› ç´ æ˜¯æä¾›å¯¹å¼‚æ„å­˜å‚¨è®¾å¤‡ä»¥åŠç¡¬ä»¶åŠ é€Ÿå™¨çš„åŸç”Ÿæ”¯æŒï¼Œè¿™å°†å…è®¸ç”¨æˆ·æ ¹æ®è‡ªå·±çš„éœ€æ±‚å¹³è¡¡æˆæœ¬å’Œæ€§èƒ½ã€‚å¦‚æœ Crimson å¯¹æ•´ä¸ªå­˜å‚¨å †æ ˆæœ‰æ›´å¥½çš„æ§åˆ¶ï¼Œé‚£ä¹ˆç®€åŒ–è§£å†³æ–¹æ¡ˆä»¥éƒ¨ç½²ç¡¬ä»¶ç»„åˆå°†æ›´åŠ çµæ´»ã€‚

SeaStore is already functional in terms of single shard reads and writes, although there are still efforts left for stability and performance improvement. The current efforts are still focused on the architecture rather than corner-case optimizations. Its design for the multiple-shard OSD is clear. Like the CyanStore, the first step is to create independent SeaStore instances per OSD shard, each running on a static partition of the storage device. The second step is to implement a shared disk space balancer to adjust the partitions dynamically, which should be fine to run in the background asynchronously because the PGs have already distributed the user IO in a pseudo random way. The SeaStore instances may not need to be equal to the number of OSD shards, adjusting this ratio is expected as the third step according to the performance analysis.  

SeaStore åœ¨å•åˆ†ç‰‡è¯»å–å’Œå†™å…¥æ–¹é¢å·²ç»æ­£å¸¸è¿è¡Œï¼Œå°½ç®¡ä»æœ‰åŠªåŠ›æé«˜ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚
ç›®å‰çš„å·¥ä½œä»ç„¶é›†ä¸­åœ¨æ¶æ„ä¸Šï¼Œè€Œä¸æ˜¯æç«¯æƒ…å†µä¸‹çš„ä¼˜åŒ–ã€‚å®ƒå¯¹å¤šåˆ†ç‰‡ OSD çš„è®¾è®¡å¾ˆæ˜ç¡®ã€‚
ä¸ CyanStore ä¸€æ ·ï¼Œç¬¬ä¸€æ­¥æ˜¯ä¸ºæ¯ä¸ª OSD åˆ†ç‰‡åˆ›å»ºç‹¬ç«‹çš„ SeaStore å®ä¾‹ï¼Œ
æ¯ä¸ªå®ä¾‹éƒ½åœ¨å­˜å‚¨è®¾å¤‡çš„é™æ€åˆ†åŒºä¸Šè¿è¡Œã€‚
ç¬¬äºŒæ­¥æ˜¯å®ç°å…±äº«ç£ç›˜ç©ºé—´å¹³è¡¡å™¨æ¥åŠ¨æ€è°ƒæ•´åˆ†åŒºï¼Œ
è¿™åœ¨åå°å¼‚æ­¥è¿è¡Œåº”è¯¥æ²¡é—®é¢˜ï¼Œå› ä¸º PG å·²ç»ä»¥ä¼ªéšæœºæ–¹å¼åˆ†å‘äº†ç”¨æˆ· IOã€‚
SeaStore å®ä¾‹å¯èƒ½ä¸éœ€è¦ç­‰äº OSD åˆ†ç‰‡çš„æ•°é‡ï¼Œæ ¹æ®æ€§èƒ½åˆ†æï¼Œè°ƒæ•´æ­¤æ¯”ç‡é¢„è®¡æ˜¯ç¬¬ä¸‰æ­¥ã€‚

# Summary and Test configurations  æ‘˜è¦å’Œæµ‹è¯•é…ç½®

In this article, 

we have introduced why and how the Ceph OSD is being refactored to **keep up** with the 

hardware trends, 
åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸ºä»€ä¹ˆä»¥åŠå¦‚ä½•é‡æ„ Ceph OSD ä»¥è·Ÿä¸Šç¡¬ä»¶è¶‹åŠ¿

design choices made, performance results for single-shard run-to-completion implementation, 
æ‰€åšçš„è®¾è®¡é€‰æ‹©ã€å•åˆ†ç‰‡è¿è¡Œåˆ°å®Œæˆå®ç°çš„æ€§èƒ½ç»“æœ

and the considerations to make Crimson OSD truly multi-core scalable.  
ä»¥åŠä½¿ Crimson OSD çœŸæ­£å…·æœ‰å¤šæ ¸å¯æ‰©å±•æ€§çš„æ³¨æ„äº‹é¡¹ã€‚



Test scenarios:
- Client: 4 FIO clients
- IO mode: random write and then random read
- Block size: 4KB
- Time: 300s X 5 times to get the average results
- IO-depth: 32 X 4 clients
- Create 1 pool using 1 replica
- 1 RBD image X 4 clients
- The size of each image is 256GB

Test environment:
- Ceph version (SHA1): 7803eb186d02bb852b95efd1a1f61f32618761d9
- Ubuntu 20.04
- GCC-12
- 1TB NVMe SSD as the BlueStore block device
- 50GB memory for MemStore and CyanStore


| ç‰¹æ€§     | RDMA                                | DPDK              | XDP                          |
| ------ | ----------------------------------- | ----------------- | ---------------------------- |
| å·¥ä½œå±‚çº§   | ç”¨æˆ·æ€                                 | ç”¨æˆ·æ€               | å†…æ ¸æ€                          |
| æ ¸å¿ƒæœºåˆ¶   | ç›´æ¥å†…å­˜è®¿é—®ï¼Œæ”¯æŒå•è¾¹æ“ä½œ                       | ç”¨æˆ·æ€è½®è¯¢ï¼Œç›´æ¥è®¿é—®ç½‘å¡      | eBPF ç¨‹åºåœ¨é©±åŠ¨å±‚å¤„ç†æ•°æ®åŒ…             |
| é›¶æ‹·è´æ”¯æŒ  | âœ… æ˜¯ï¼ˆç¡¬ä»¶æ”¯æŒï¼‰                           | âœ… æ˜¯ï¼ˆç”¨æˆ·æ€å†…å­˜æ± ï¼‰       | âœ… æ˜¯ï¼ˆAF_XDP æ¨¡å¼ï¼‰               |
| å»¶è¿Ÿè¡¨ç°   | æä½ï¼ˆÎ¼s çº§ï¼Œé€‚ç”¨äº HPC å’Œå­˜å‚¨ï¼‰                | ä½ï¼ˆé€‚ç”¨äºé«˜åååœºæ™¯ï¼‰       | ä½ï¼ˆé€‚ç”¨äº L2/L3 å±‚å¿«é€Ÿå¤„ç†ï¼‰           |
| å…¸å‹åº”ç”¨åœºæ™¯ | åˆ†å¸ƒå¼å­˜å‚¨ã€AI è®­ç»ƒã€HPC                     | NFVã€SDNã€ç½‘ç»œè´Ÿè½½å‡è¡¡    | DDoS é˜²æŠ¤ã€è¾¹ç¼˜è¿‡æ»¤ã€å†…æ ¸æ€è´Ÿè½½å‡è¡¡         |
| ç¡¬ä»¶ä¾èµ–   | é«˜ï¼ˆéœ€æ”¯æŒ RDMA çš„ NICï¼Œå¦‚ RoCEã€InfiniBandï¼‰ | ä¸­ï¼ˆéœ€æ”¯æŒ DPDK çš„ NICï¼‰ | ä½ï¼ˆå¤§å¤šæ•°ç½‘å¡æ”¯æŒï¼Œéƒ¨åˆ†åŠŸèƒ½éœ€é©±åŠ¨æ”¯æŒï¼‰         |
| æ§åˆ¶é¢å¤„ç†  | ç”¨æˆ·æ€ APIï¼Œéœ€æ³¨å†Œå†…å­˜å’Œé˜Ÿåˆ—å¯¹                   | ç”¨æˆ·æ€çº¿ç¨‹æ§åˆ¶ï¼Œéœ€ç®¡ç†å†…å­˜æ± å’Œé˜Ÿåˆ— | å†…æ ¸æ€ eBPF ç®¡ç†ï¼Œç”¨æˆ·æ€å¯é€šè¿‡ AF_XDP äº¤äº’ |
3FS é›†ç¾¤ç”± 180 ä¸ªå­˜å‚¨èŠ‚ç‚¹ç»„æˆï¼Œæ¯ä¸ªå­˜å‚¨èŠ‚ç‚¹é…å¤‡ 2Ã—200 Gbps InfiniBand ç½‘å¡å’Œ 16 ä¸ª 14TiB NVMe SSDã€‚180 èŠ‚ç‚¹èšåˆè¯»å–ååé‡è¾¾åˆ°çº¦ 6.6 TiB/sã€‚

3FS ä¸»è¦æœ‰ 4 å¤§æ ¸å¿ƒï¼šCluster Managerï¼ŒMetadata Serviceï¼ŒStorage Service å’Œ Clientï¼Œå„ä¸ªæ¨¡å—ä¹‹é—´éƒ½ä½¿ç”¨ RDMA ç½‘ç»œè¿æ¥

https://deepseek.csdn.net/67c6d414d649b06b61c7d28a.html?utm_source=chatgpt.com
å†…æ ¸æ—è·¯ï¼ˆKernel Bypassï¼‰æ˜¯ä¸€ç§é«˜æ€§èƒ½è®¡ç®—å’Œç½‘ç»œé€šä¿¡é¢†åŸŸçš„é‡è¦æŠ€æœ¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç»•è¿‡æ“ä½œç³»ç»Ÿå†…æ ¸


ä»¥ä¸‹æ˜¯é’ˆå¯¹åˆå­¦è€…ä»é›¶åŸºç¡€åˆ°æŒæ¡å†…æ ¸æ—è·¯ï¼ˆKernel Bypassï¼‰æŠ€æœ¯çš„ç³»ç»Ÿå­¦ä¹ è®¡åˆ’ï¼Œæ¶µç›–ç†è®ºçŸ¥è¯†ã€å®æˆ˜æŠ€èƒ½å’Œé¡¹ç›®å®è·µï¼Œé€‚ç”¨äºç½‘ç»œå¼€å‘ã€ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æˆ–é«˜æ€§èƒ½è®¡ç®—é¢†åŸŸçš„å·¥ç¨‹å¸ˆã€‚

---

## ğŸ§­ å­¦ä¹ è·¯å¾„æ¦‚è§ˆ

|é˜¶æ®µ|ç›®æ ‡|å…³é”®å†…å®¹|æ¨èæ—¶é—´|
|---|---|---|---|
|**é˜¶æ®µ 1**|æ‰“ä¸‹åŸºç¡€|æ“ä½œç³»ç»Ÿã€ç½‘ç»œåè®®ã€LinuxåŸºç¡€|1â€“2 å‘¨|
|**é˜¶æ®µ 2**|ç†è§£å†…æ ¸æ—è·¯åŸç†|DPDKã€XDPã€UIOã€NUMAã€é›¶æ‹·è´|2â€“3 å‘¨|
|**é˜¶æ®µ 3**|åŠ¨æ‰‹å®è·µ|æ­å»ºç¯å¢ƒã€è¿è¡Œç¤ºä¾‹ã€æ€§èƒ½æµ‹è¯•|2â€“4 å‘¨|
|**é˜¶æ®µ 4**|é¡¹ç›®å®æˆ˜|æ„å»ºé«˜æ€§èƒ½ç½‘ç»œåº”ç”¨ã€è°ƒä¼˜|4â€“6 å‘¨|
|**é˜¶æ®µ 5**|è¿›é˜¶ä¸æ‹“å±•|eBPFã€RDMAã€äº‘åŸç”Ÿç½‘ç»œ|æŒç»­å­¦ä¹ |

---

## ğŸ§© é˜¶æ®µ 1ï¼šåŸºç¡€çŸ¥è¯†

**ç›®æ ‡**ï¼šæŒæ¡æ“ä½œç³»ç»Ÿå’Œç½‘ç»œé€šä¿¡çš„åŸºæœ¬æ¦‚å¿µï¼Œä¸ºæ·±å…¥å­¦ä¹ å†…æ ¸æ—è·¯æŠ€æœ¯æ‰“ä¸‹åšå®åŸºç¡€ã€‚

**å­¦ä¹ å†…å®¹**ï¼š

- æ“ä½œç³»ç»ŸåŸç†ï¼šè¿›ç¨‹ä¸çº¿ç¨‹ã€å†…å­˜ç®¡ç†ã€ç³»ç»Ÿè°ƒç”¨ã€ä¸Šä¸‹æ–‡åˆ‡æ¢ç­‰ã€‚
    
- è®¡ç®—æœºç½‘ç»œåŸºç¡€ï¼šTCP/IPåè®®æ ˆã€Socketç¼–ç¨‹ã€ç½‘ç»œI/Oæ¨¡å‹ã€‚
    
- LinuxåŸºç¡€ï¼šShellå‘½ä»¤ã€æ–‡ä»¶ç³»ç»Ÿã€è¿›ç¨‹ç®¡ç†ã€ç½‘ç»œé…ç½®ã€‚([The Byte](https://thebyte.com.cn/content/chapter1/dpdk.html?utm_source=chatgpt.com "å†…æ ¸æ—è·¯æŠ€æœ¯ä»‹ç» | æ·±å…¥æ¶æ„åŸç†ä¸å®è·µ"))
    

**æ¨èèµ„æº**ï¼š

- ã€Šæ“ä½œç³»ç»Ÿæ¦‚å¿µã€‹ by Abraham Silberschatz
    
- ã€Šè®¡ç®—æœºç½‘ç»œï¼šè‡ªé¡¶å‘ä¸‹æ–¹æ³•ã€‹ by James F. Kurose
    
- Linuxå®˜æ–¹æ–‡æ¡£å’Œåœ¨çº¿æ•™ç¨‹
    

---

## âš™ï¸ é˜¶æ®µ 2ï¼šç†è§£å†…æ ¸æ—è·¯åŸç†

**ç›®æ ‡**ï¼šæ·±å…¥ç†è§£å†…æ ¸æ—è·¯æŠ€æœ¯çš„åŸç†ã€ä¼˜åŠ¿ä»¥åŠå¸¸è§å®ç°æ–¹å¼ã€‚

**å­¦ä¹ å†…å®¹**ï¼š

- å†…æ ¸ä¸ç”¨æˆ·ç©ºé—´çš„é€šä¿¡æœºåˆ¶ã€‚
    
- å†…æ ¸æ—è·¯çš„åŠ¨æœºï¼šå‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢ã€é™ä½å»¶è¿Ÿã€æé«˜ååé‡ã€‚
    
- ä¸»è¦æŠ€æœ¯ï¼š
    
    - DPDKï¼ˆData Plane Development Kitï¼‰ï¼šç”¨æˆ·æ€é«˜é€Ÿæ•°æ®åŒ…å¤„ç†åº“ã€‚
        
    - XDPï¼ˆeXpress Data Pathï¼‰ï¼šåŸºäºeBPFçš„å†…æ ¸æ•°æ®è·¯å¾„ä¼˜åŒ–æŠ€æœ¯ã€‚
        
    - UIOï¼ˆUser-space I/Oï¼‰ï¼šç”¨æˆ·ç©ºé—´é©±åŠ¨ç¨‹åºæ¥å£ã€‚
        
    - NUMAï¼ˆéç»Ÿä¸€å†…å­˜è®¿é—®ï¼‰ï¼šä¼˜åŒ–å¤šæ ¸å¤„ç†å™¨çš„å†…å­˜è®¿é—®ã€‚
        
    - é›¶æ‹·è´æŠ€æœ¯ï¼šå‡å°‘æ•°æ®å¤åˆ¶ï¼Œæé«˜I/Oæ•ˆç‡ã€‚([The Byte](https://thebyte.com.cn/content/chapter1/dpdk.html?utm_source=chatgpt.com "å†…æ ¸æ—è·¯æŠ€æœ¯ä»‹ç» | æ·±å…¥æ¶æ„åŸç†ä¸å®è·µ"), [åä¸ºå¼€å‘è€…ç©ºé—´](https://huaweicloud.csdn.net/63566835d3efff3090b5de1f.html?utm_source=chatgpt.com "Linuxæ€§èƒ½ä¼˜åŒ–ï¼ˆä¹ï¼‰â€”â€”Kernel Bypass_linux_å¤©å±±è€å¦–-åä¸ºå¼€å‘è€…ç©ºé—´"))
        

**æ¨èèµ„æº**ï¼š

- DPDKå®˜æ–¹æ–‡æ¡£ï¼š
    
- XDPæ•™ç¨‹å’Œç¤ºä¾‹ä»£ç ï¼š
    
- UIOé©±åŠ¨å¼€å‘æŒ‡å—ï¼š([åä¸ºå¼€å‘è€…ç©ºé—´](https://huaweicloud.csdn.net/63566835d3efff3090b5de1f.html?utm_source=chatgpt.com "Linuxæ€§èƒ½ä¼˜åŒ–ï¼ˆä¹ï¼‰â€”â€”Kernel Bypass_linux_å¤©å±±è€å¦–-åä¸ºå¼€å‘è€…ç©ºé—´"))
    

---

## ğŸ› ï¸ é˜¶æ®µ 3ï¼šåŠ¨æ‰‹å®è·µ

**ç›®æ ‡**ï¼šæ­å»ºå®éªŒç¯å¢ƒï¼Œè¿è¡Œå†…æ ¸æ—è·¯æŠ€æœ¯çš„ç¤ºä¾‹ç¨‹åºï¼Œè¿›è¡Œæ€§èƒ½æµ‹è¯•ã€‚

**å®è·µå†…å®¹**ï¼š

- æ­å»ºDPDKå¼€å‘ç¯å¢ƒï¼šé…ç½®HugePagesã€ç»‘å®šç½‘å¡é©±åŠ¨ã€‚
    
- è¿è¡ŒDPDKç¤ºä¾‹ç¨‹åºï¼šå¦‚l2fwdï¼ˆLayer 2 Forwardingï¼‰ã€‚
    
- ä½¿ç”¨XDPåŠ è½½eBPFç¨‹åºï¼Œå®ç°ç®€å•çš„æ•°æ®åŒ…è¿‡æ»¤ã€‚
    
- æ€§èƒ½æµ‹è¯•ï¼šä½¿ç”¨å·¥å…·å¦‚iperfã€pktgenè¿›è¡Œç½‘ç»œæ€§èƒ½è¯„ä¼°ã€‚([åšå®¢å›­](https://www.cnblogs.com/xueqiuqiu/articles/12242844.html?utm_source=chatgpt.com "DPDK å®Œå…¨å†…æ ¸æ—è·¯æŠ€æœ¯å®ç° - é›ªçƒçƒ - åšå®¢å›­"))
    

**æ¨èèµ„æº**ï¼š

- DPDKå®˜æ–¹ç¤ºä¾‹ä»£ç ï¼š
    
- XDPæ•™ç¨‹å’Œç¤ºä¾‹ä»£ç ï¼š
    
- Linuxç½‘ç»œæ€§èƒ½æµ‹è¯•å·¥å…·æŒ‡å—ï¼š
    

---

## ğŸš€ é˜¶æ®µ 4ï¼šé¡¹ç›®å®æˆ˜

**ç›®æ ‡**ï¼šåº”ç”¨æ‰€å­¦çŸ¥è¯†ï¼Œå¼€å‘é«˜æ€§èƒ½ç½‘ç»œåº”ç”¨ï¼Œè¿›è¡Œæ€§èƒ½ä¼˜åŒ–å’Œè°ƒè¯•ã€‚

**é¡¹ç›®å»ºè®®**ï¼š

- å¼€å‘åŸºäºDPDKçš„ç½‘ç»œè´Ÿè½½å‡è¡¡å™¨ã€‚
    
- å®ç°åŸºäºXDPçš„é˜²ç«å¢™æˆ–æ•°æ®åŒ…è¿‡æ»¤å™¨ã€‚
    
- æ„å»ºé«˜æ€§èƒ½ç½‘ç»œç›‘æ§ç³»ç»Ÿï¼Œå®æ—¶åˆ†æç½‘ç»œæµé‡ã€‚([åä¸ºå¼€å‘è€…ç©ºé—´](https://huaweicloud.csdn.net/63566835d3efff3090b5de1f.html?utm_source=chatgpt.com "Linuxæ€§èƒ½ä¼˜åŒ–ï¼ˆä¹ï¼‰â€”â€”Kernel Bypass_linux_å¤©å±±è€å¦–-åä¸ºå¼€å‘è€…ç©ºé—´"))
    

**ä¼˜åŒ–æŠ€å·§**ï¼š

- ä½¿ç”¨å¤§é¡µå†…å­˜ï¼ˆHugePagesï¼‰å‡å°‘TLB missã€‚
    
- ç»‘å®šçº¿ç¨‹åˆ°ç‰¹å®šCPUæ ¸å¿ƒï¼Œå‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢ã€‚
    
- åˆ©ç”¨NUMAæ¶æ„ä¼˜åŒ–å†…å­˜è®¿é—®ã€‚
    
- é‡‡ç”¨é›¶æ‹·è´æŠ€æœ¯ï¼Œå‡å°‘æ•°æ®å¤åˆ¶ã€‚([The Byte](https://thebyte.com.cn/content/chapter1/dpdk.html?utm_source=chatgpt.com "å†…æ ¸æ—è·¯æŠ€æœ¯ä»‹ç» | æ·±å…¥æ¶æ„åŸç†ä¸å®è·µ"), [åšå®¢å›­](https://www.cnblogs.com/xueqiuqiu/articles/12242844.html?utm_source=chatgpt.com "DPDK å®Œå…¨å†…æ ¸æ—è·¯æŠ€æœ¯å®ç° - é›ªçƒçƒ - åšå®¢å›­"), [ç»´åŸºç™¾ç§‘](https://zh.wikipedia.org/wiki/%E9%9B%B6%E5%A4%8D%E5%88%B6?utm_source=chatgpt.com "é›¶å¤åˆ¶"))
    

---

## ğŸŒ é˜¶æ®µ 5ï¼šè¿›é˜¶ä¸æ‹“å±•

**ç›®æ ‡**ï¼šæ¢ç´¢å†…æ ¸æ—è·¯æŠ€æœ¯çš„é«˜çº§åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚

**å­¦ä¹ å†…å®¹**ï¼š

- eBPFï¼ˆExtended Berkeley Packet Filterï¼‰ï¼šå†…æ ¸å¯ç¼–ç¨‹æ€§å¢å¼ºã€‚
    
- RDMAï¼ˆRemote Direct Memory Accessï¼‰ï¼šè¿œç¨‹ç›´æ¥å†…å­˜è®¿é—®æŠ€æœ¯ã€‚
    
- äº‘åŸç”Ÿç½‘ç»œï¼šå¦‚Ciliumã€Kube-Proxyçš„æ›¿ä»£æ–¹æ¡ˆã€‚
    
- å®‰å…¨æ€§ä¸éš”ç¦»æ€§ï¼šå†…æ ¸æ—è·¯æŠ€æœ¯åœ¨å®‰å…¨é¢†åŸŸçš„æŒ‘æˆ˜ä¸å¯¹ç­–ã€‚([ç»´åŸºç™¾ç§‘](https://zh.wikipedia.org/wiki/%E9%9B%B6%E5%A4%8D%E5%88%B6?utm_source=chatgpt.com "é›¶å¤åˆ¶"))
    

**æ¨èèµ„æº**ï¼š

- eBPFå®˜æ–¹æ–‡æ¡£å’Œæ•™ç¨‹ï¼š
    
- RDMAæŠ€æœ¯ç™½çš®ä¹¦å’Œå¼€å‘æŒ‡å—ï¼š
    
- äº‘åŸç”Ÿç½‘ç»œé¡¹ç›®æ–‡æ¡£ï¼š([åä¸ºå¼€å‘è€…ç©ºé—´](https://huaweicloud.csdn.net/63566835d3efff3090b5de1f.html?utm_source=chatgpt.com "Linuxæ€§èƒ½ä¼˜åŒ–ï¼ˆä¹ï¼‰â€”â€”Kernel Bypass_linux_å¤©å±±è€å¦–-åä¸ºå¼€å‘è€…ç©ºé—´"), [åšå®¢å›­](https://www.cnblogs.com/xueqiuqiu/articles/12242844.html?utm_source=chatgpt.com "DPDK å®Œå…¨å†…æ ¸æ—è·¯æŠ€æœ¯å®ç° - é›ªçƒçƒ - åšå®¢å›­"))
    

---

## ğŸ“š æ¨èå­¦ä¹ èµ„æº

- DPDKå®˜æ–¹æ–‡æ¡£ï¼š
    
- XDPæ•™ç¨‹å’Œç¤ºä¾‹ä»£ç ï¼š
    
- ã€ŠLinuxé«˜æ€§èƒ½æœåŠ¡å™¨ç¼–ç¨‹ã€‹ by æ¸¸åŒ
    
- ã€Šæ·±å…¥ç†è§£Linuxç½‘ç»œå†…å¹•ã€‹ by Christian Benvenuti
    

---

é€šè¿‡ä»¥ä¸Šç³»ç»Ÿçš„å­¦ä¹ è®¡åˆ’ï¼Œæ‚¨å°†èƒ½å¤Ÿä»é›¶åŸºç¡€é€æ­¥æŒæ¡å†…æ ¸æ—è·¯æŠ€æœ¯ï¼Œå…·å¤‡å¼€å‘é«˜æ€§èƒ½ç½‘ç»œåº”ç”¨çš„èƒ½åŠ›ã€‚åœ¨å®è·µè¿‡ç¨‹ä¸­ï¼Œå»ºè®®å¤šå‚ä¸å¼€æºç¤¾åŒºï¼Œé˜…è¯»ç›¸å…³é¡¹ç›®çš„æºä»£ç ï¼Œæå‡å®æˆ˜ç»éªŒã€‚

å¦‚æœæ‚¨æœ‰å…´è¶£ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æ¨èä¸€äº›å…·ä½“çš„å¼€æºé¡¹ç›®å’Œå®è·µæ¡ˆä¾‹ï¼Œå¸®åŠ©æ‚¨æ›´å¥½åœ°åº”ç”¨æ‰€å­¦çŸ¥è¯†ã€‚



### åˆ°åº•ä»€ä¹ˆæ˜¯TCP/IPåè®®æ ˆï¼Œçœ‹å®Œè¿™ç¯‡ä½ å°±æ˜ç™½ï¼
https://www.caimore.com/NEWS-show-1401.html

# RocksDB æºç åˆ†æ â€“ I/O
https://youjiali1995.github.io/rocksdb/io/


page cache æ˜¯ä»€ä¹ˆæ•°æ®ç»“æ„ç»„æˆçš„ï¼Ÿå¦‚ä½•å­˜å‚¨ï¼Œå¦‚ä½•æŸ¥æ‰¾ï¼Œ

ä»æ¶æ„å¸ˆè§’åº¦å›ç­”ï¼Œå¹¶ä¸”è€ƒè™‘ç»†èŠ‚ 
1. é¿å…èƒŒè¯µ å…«è‚¡æ–‡ï¼Œåªæ˜¯æ¦‚å¿µï¼Œæ²¡è€ƒè™‘å®é™…é¡¹ç›®æƒ…å†µå’ŒçŸ¥è¯†æƒ…å†µ
2. èƒŒæ™¯ å¤§å­¦è¯¾ç¨‹å’Œå®é™…10 å¹´é¡¹ç›®å¼€å‘ç»éªŒ

https://youjiali1995.github.io/essay/2023-summary/æˆ‘åœ¨ PingCAP ä¸€å…±å¾…äº†3å¹´8ä¸ªæœˆï¼Œæ˜¯æˆ‘ç›®å‰èŒä¸šç”Ÿæ¶¯ï¼ˆ6å¹´7ä¸ªæœˆï¼‰çš„ 55.7%ã€‚ç¦»å¼€ PingCAP å€’ä¸æ˜¯ä¸çœ‹å¥½å®ƒçš„å‘å±•ï¼Œä¹Ÿä¸æ˜¯è‡ªå·±å‘å±•çš„ä¸å¥½ï¼Œè€Œæ˜¯æ•°æ®åº“å¯¹æˆ‘æ¥è¯´å¤ªéš¾äº†ã€‚

æˆ‘è®¤ä¸ºå®ƒä»æ˜¯ä¸­å›½ infra é¢†åŸŸæŠ€æœ¯æ°´å¹³æœ€é«˜ã€æœ€å°Šé‡äººçš„å…¬å¸ä¹‹ä¸€ã€‚PingCAP æœ‰ç€ä¸é”™çš„ä¼ä¸šæ–‡åŒ–ä¸” WLBï¼›æœ‰ç€å®Œå–„çš„æ–°äººåŸ¹å…»ä½“ç³»ä¹Ÿå¯¹æ–°äººéå¸¸è€å¿ƒï¼Œæ–°äººå‚ä¸åˆ°å¤æ‚ä¸”æˆç†Ÿçš„é¡¹ç›®ä¸­æ¥ï¼Œä¼šåœ¨ä»£ç ã€æµ‹è¯•ã€å¯è§‚æµ‹æ€§ã€æ’æŸ¥å’Œè§£å†³é—®é¢˜ã€åˆä½œç­‰å„ä¸ªæ–¹é¢å¾—åˆ°åŸ¹å…»å’Œæå‡ï¼›


è¿™ä¸ªé¢†åŸŸä¹Ÿè¿‡äºå†…å·ï¼Œæ¯”å¦‚å· TPC-C ç­‰ benchmarkï¼Œå¾ˆå¯èƒ½è¿‡åº¦æ»¡è¶³äº†å¸‚åœºçš„éœ€è¦ï¼Œå‡ºç°äº†æ€§èƒ½çš„è¿‡åº¦ä¾›ç»™ï¼Œæ—¢åˆ›é€ ä¸äº†è¡Œä¸šä»·å€¼ä¹Ÿä¸èƒ½ä¸ºç”¨æˆ·åˆ›é€ ä»·å€¼ã€‚æœ€æœ‰æ•ˆçš„ç«äº‰æ–¹å¼å¹¶ä¸æ˜¯å·ï¼Œè€Œæ˜¯æä¾›å…¨æ–°çš„åŠŸèƒ½å’Œä½“éªŒåœ¨å¦ä¸€ä¸ªç»´åº¦ç«äº‰ï¼Œå®ç°é™ç»´æ‰“å‡»ã€‚äº§å“å¦‚æ­¤ï¼Œäººä¹Ÿå¦‚æ­¤ã€‚

è‹¦éš¾æœ¬èº«æ²¡æœ‰ä»»ä½•æ„ä¹‰ï¼Œè‹¦éš¾å°±æ˜¯è‹¦éš¾ã€‚éš¾çš„äº‹æƒ…ä¸ä»£è¡¨å®ƒæœ¬èº«éœ€è¦é‚£ä¹ˆéš¾ï¼Œæ›´å¯èƒ½æ˜¯åŸºç¡€ã€æ–¹å‘é”™äº†ï¼ŒåŸºç¡€è¦æ‰“ç‰¢ï¼Œè¦åœ¨æ­£ç¡®çš„æ–¹å‘ã€æ¡†æ¶ä¸Šåšï¼Œè€Œä¸æ˜¯åœ¨ä¸æ­£ç¡®çš„æ¡†æ¶ä¸Šè¡¥ï¼Œä»€ä¹ˆéƒ½æƒ³åšå¾ˆå¯èƒ½ä»€ä¹ˆéƒ½åšä¸å¥½ï¼Œå£®å£«æ–­è…•çš„é­„åŠ›å’Œå†³å¿ƒä¸æ˜¯æ¯ä¸ªäººéƒ½æœ‰çš„ã€‚


å¼‚æ­¥ç¼–ç¨‹æ ¸å¿ƒæ˜¯ event loopï¼ŒåŸºæœ¬æ¨¡å‹éƒ½æ˜¯å•ä¸ª/å¤šä¸ªçº¿ç¨‹/è¿›ç¨‹é˜»å¡åœ¨Â `epoll`Â è¿™ç§ I/O å¤šè·¯å¤ç”¨çš„ç³»ç»Ÿè°ƒç”¨ï¼Œæœ‰äº‹ä»¶å°±ç»ªä¸€èˆ¬å°±åœ¨å½“å‰çº¿ç¨‹ç›´æ¥å¤„ç†äº†ï¼Œé•¿æ—¶é—´çš„å·¥ä½œä¼šæ‰”åˆ°å•ç‹¬çš„çº¿ç¨‹æ± é‡Œæ‰§è¡Œï¼Œé˜²æ­¢é˜»å¡ event loopã€‚Rust æŠŠè¿™ä¸€æ•´å¥—æ¨¡å‹æ‹†æˆäº†å¤šä¸ªéƒ¨åˆ†